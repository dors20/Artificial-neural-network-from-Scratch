{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96 entries, 0 to 95\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Community       96 non-null     int64  \n",
      " 1   Age             89 non-null     float64\n",
      " 2   Weight          85 non-null     float64\n",
      " 3   Delivery phase  92 non-null     float64\n",
      " 4   HB              77 non-null     float64\n",
      " 5   IFA             96 non-null     int64  \n",
      " 6   BP              81 non-null     float64\n",
      " 7   Education       93 non-null     float64\n",
      " 8   Residence       94 non-null     float64\n",
      " 9   Result          96 non-null     int64  \n",
      "dtypes: float64(7), int64(3)\n",
      "memory usage: 7.6 KB\n",
      "Community:4\n",
      "Age:14\n",
      "Weight:28\n",
      "Delivery phase:2\n",
      "HB:22\n",
      "IFA:2\n",
      "BP:20\n",
      "Education:1\n",
      "Residence:2\n",
      "Result:2\n",
      "9.076623376623376\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "#To reset seed so same set of numbers are produced\n",
    "np.random.seed(0)\n",
    "\n",
    "#inputing the dataset\n",
    "data = pd.read_csv('LBW_Dataset.csv') \n",
    "data.head(5)\n",
    "data.info()\n",
    "\n",
    "#Count of null values column wise\n",
    "data.isnull().sum()\n",
    "\n",
    "#Inspect unique values in every feature\n",
    "for col in data.columns:\n",
    "    print(\"{}:{}\".format(col,data[col].nunique()))\n",
    "\n",
    "#Basic statistics feautre wise\n",
    "data.describe()\n",
    "\n",
    "#NO null values in community column\n",
    "data['Community'].value_counts()\n",
    "\n",
    "data['Residence'].value_counts()\n",
    "\n",
    "# FIll Age ,Weight ,HB and BP  Null values with mean\n",
    "# Delivery phase with 1 since the coulmn takes binary values\n",
    "# Education with 5\n",
    "#Residence with 1\n",
    "\n",
    "#returns all the features\n",
    "data.columns\n",
    "\n",
    "#used to fill the \"HB\" feature by mean\n",
    "x=data['HB'].mean()\n",
    "print(x)\n",
    "\n",
    "# A dict containing column wise values used to fill the null values\n",
    "values = { 'Age':data['Age'].mean(), 'Weight':data['Weight'].mean(), 'Delivery phase':1, 'HB':9.0, 'BP':data['BP'].mean(),\n",
    "       'Education':5, 'Residence':1}\n",
    "data = data.fillna(value=values)\n",
    "data.head(3)\n",
    "data.isnull().sum() #no nulls values the dataset is now cleaned\n",
    "\n",
    "#Writing the cleaned output to new file\n",
    "data.to_csv(\"LBW_Dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "class NN:\n",
    "    \n",
    "    ''' X and Y are dataframes'''\n",
    "    def __init__(self,numHlayers=2):\n",
    "        \n",
    "        #number of hidden layers the ANN has\n",
    "        self.numHlayers = numHlayers\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by talking x_train and y_train samples as input\n",
    "        '''\n",
    "        input1=X\n",
    "        result1=Y\n",
    "        \n",
    "        #If the ANN has two hidden layer we initialize it calling ANN_layer class\n",
    "        \n",
    "        if(self.numHlayers==2):\n",
    "            \n",
    "            #First layer is initialized with parameter 1 :Number of inputs\n",
    "            #                                parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "            self.Hlayer1 =ANN_layers(7,24)\n",
    "            self.Hlayer2 =ANN_layers(24,24)\n",
    "            self.Output_layer =ANN_layers(24,2)\n",
    "            \n",
    "            #Between each layer we have the activation function class with a \n",
    "            #wide range of activation functions \n",
    "            \n",
    "            self.act1=activation()\n",
    "            self.act2=activation()\n",
    "            \n",
    "            # using the softmax function on the final layer \n",
    "            self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "            #So for optimizer we can choose between Adam and SGD optimizer\n",
    "            #Creating optimizer\n",
    "            self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "            \n",
    "            #Loop to train in\n",
    "            for epoch in range(10001):\n",
    "                \n",
    "                #pass input X through first layer\n",
    "                self.Hlayer1.fpropogation(input1)\n",
    "                \n",
    "                #Output of first layer passes through the firstactivation function\n",
    "                self.act1.fRelu(self.Hlayer1.output)\n",
    "                \n",
    "                #Output of first activation is sent in forward pass to second layer\n",
    "                self.Hlayer2.fpropogation(self.act1.output)\n",
    "                \n",
    "                 #Output ofsecond layer passes through the next activation function\n",
    "                self.act2.fRelu(self.Hlayer2.output)\n",
    "                \n",
    "                # Perform a forward pass through the activation/loss function\n",
    "                # takes the output of second dense layer here and returns loss\n",
    "                self.Output_layer.fpropogation(self.act2.output)\n",
    "                loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "                #Argmax returns the predictions for the foward pass\n",
    "                prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "                #calculate values along first axis\n",
    "                if len(result1.shape)==2:\n",
    "                    result1=np.argmax(result1,axis=1)\n",
    "                #returns the accuracy of our predictions in this pass\n",
    "                accuracy= np.mean(prediction==result1)\n",
    "                \n",
    "                #priniting for every 100Th epoch \n",
    "                if not epoch %100:\n",
    "                    print(f'epoch:{epoch} ,'+\n",
    "                          f'accuracy:{accuracy:.3f} ,'+\n",
    "                          f'loss:{loss} ,'+\n",
    "                          f'lr:{self.optimize.curr_l_rate},')\n",
    "                    \n",
    "                #back propogation \n",
    "                #The values are propogated backwards and the dinputs are updated\n",
    "                self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "                self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "                self.act2.back(self.Output_layer.dinputs)\n",
    "                self.Hlayer2.Bpropogation(self.act2.dinputs)\n",
    "                self.act1.back(self.Hlayer2.dinputs)\n",
    "                self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "                #Update in weights and the biases\n",
    "                self.optimize.initial_update_param()\n",
    "                self.optimize.update_params(self.Hlayer1)\n",
    "                self.optimize.update_params(self.Hlayer2)\n",
    "                self.optimize.update_params(self.Output_layer)\n",
    "                self.optimize.post_update_params()\n",
    "        \n",
    "        \n",
    "        #If ANN has 1 hidden layer we initialize it calling ANN_layer()\n",
    "        \n",
    "        if(self.numHlayers==1):\n",
    "            \n",
    "            # layers are initialized with parameter 1 :Number of inputs\n",
    "            #                             parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "            \n",
    "            self.Hlayer1 =ANN_layers(7,64)\n",
    "            self.Output_layer =ANN_layers(64,2)\n",
    "            #Between each layer we have the activation function class with a \n",
    "            #wide range of activation functions \n",
    "            \n",
    "            self.act1=activation()\n",
    "            \n",
    "            # using the softmax function on the final layer\n",
    "            self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "            #So for optimizer we can choose between Adam and SGD optimizer\n",
    "            #Creating optimizer\n",
    "            self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "            #Loop to train in\n",
    "            for epoch in range(11001):\n",
    "                \n",
    "                \n",
    "                #pass input X through first layer\n",
    "                #Output of first layer passes through the firstactivation function\n",
    "                # Perform a forward pass through the activation/loss function\n",
    "                # takes the output of second dense layer here and returns loss\n",
    "                self.Hlayer1.fpropogation(input1)\n",
    "                self.act1.fRelu(self.Hlayer1.output)\n",
    "                self.Output_layer.fpropogation(self.act1.output)\n",
    "                loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "                #Argmax returns the predictions for the foward pass\n",
    "                prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "                #calculate values along first axis\n",
    "                if len(result1.shape)==2:\n",
    "                    result1=np.argmax(result1,axis=1)\n",
    "               \n",
    "                #returns the accuracy of our predictions in this pass\n",
    "                accuracy= np.mean(prediction==result1)\n",
    "               \n",
    "                \n",
    "                #priniting for every 100Th epoch\n",
    "                if not epoch %100:\n",
    "                    print(f'epoch:{epoch} ,'+\n",
    "                          f'accuracy:{accuracy:.3f} ,'+\n",
    "                          f'loss:{loss} ,'+\n",
    "                          f'lr:{self.optimize.curr_l_rate},')\n",
    "                \n",
    "                \n",
    "                #back propogation \n",
    "                #The values are propogated backwards and the dinputs are updated\n",
    "                \n",
    "                self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "                self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "                self.act1.back(self.Output_layer.dinputs)\n",
    "                self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "                #Update in weights and the biases\n",
    "\n",
    "                self.optimize.initial_update_param()\n",
    "                self.optimize.update_params(self.Hlayer1)\n",
    "                self.optimize.update_params(self.Output_layer)\n",
    "                self.optimize.post_update_params()\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "    def predict(self,X,Y):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \n",
    "        Only Forward pass to be performed in already trained model\n",
    "        \"\"\"\n",
    "        \n",
    "        #Only Forward pass to be performed in already trained model\n",
    "        x_test=X\n",
    "        y_test=Y\n",
    "        #If ANN has 2 hidden layer we call the  ANN_layer() objects\n",
    "        if(self.numHlayers==2):\n",
    "            #pass input to the first layer \n",
    "            self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "            #layer 1 activation func\n",
    "            self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "            #output of act1 to 2nd layer\n",
    "            self.Hlayer2.fpropogation(self.act1.output)\n",
    "            \n",
    "            #layer 2 activation func\n",
    "            self.act2.fRelu(self.Hlayer2.output)\n",
    "            self.Output_layer.fpropogation(self.act2.output)\n",
    "            \n",
    "            #loss/activation for output layer\n",
    "            loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "            #Argmax returns our models predictions\n",
    "            prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "            if len(y_test.shape)==2:\n",
    "                result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "            #Accuracy out model has attained\n",
    "            accuracy= np.mean(prediction==y_test)\n",
    "            print(f'accuracy:{accuracy:.3f} ,'+\n",
    "                  f'lr:{self.optimize.curr_l_rate},')\n",
    "            \n",
    "            #return the prediction\n",
    "            yhat=prediction\n",
    "            return yhat\n",
    "       \n",
    "    \n",
    "        #If ANN has 1 hidden layer we call the  ANN_layer() objects\n",
    "        if(self.numHlayers==1):\n",
    "            #pass input to the first layer \n",
    "            self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "            #layer 1 activation func\n",
    "            self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "            self.Output_layer.fpropogation(self.act1.output)\n",
    "            #loss/activation for output layer\n",
    "            loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "            #Argmax returns our models predictions\n",
    "            prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "            if len(y_test.shape)==2:\n",
    "                result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "            #Accuracy out model has attained\n",
    "            accuracy= np.mean(prediction==y_test)\n",
    "            print(f'accuracy:{accuracy:.3f} ,'+\n",
    "                  f'lr:{self.optimize.curr_l_rate},')\n",
    "            #return the prediction\n",
    "            yhat=prediction\n",
    "            return yhat\n",
    "        \n",
    "    \n",
    "    def CM(self,y_test,y_test_obs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Since our prediction are already given in list np.array we dont \n",
    "        need to convert confidence values\n",
    "        \"\"\"\n",
    "        #2x2 matrix \n",
    "        cm=np.array([[0,0],[0,0]])\n",
    "        fp=0 #false positive\n",
    "        fn=0 #false negative\n",
    "        tp=0 #true Positive\n",
    "        tn=0 #true negative\n",
    "        \n",
    "        #loop over number of samples\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "        \n",
    "        p= tp/(tp+fp) #precision\n",
    "        r=tp/(tp+fn)  #recall\n",
    "        f1=(2*p*r)/(p+r) #f1 score\n",
    "        \n",
    "        \n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        \n",
    "        \n",
    "                \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer class so each layer can be intilialised as an object \n",
    "class ANN_layers:\n",
    "    \n",
    "    # layer properties\n",
    "    def __init__(self,num_inputs ,num_cells):\n",
    "        \n",
    "        #initializing weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(num_inputs,num_cells) #in range[-1,1]\n",
    "        self.biases = np.zeros((1,num_cells))\n",
    "        #self.w_cahce=np.zeros_like(self.weights)\n",
    "    \n",
    "    #forward pass\n",
    "    def fpropogation(self , inputs):\n",
    "        \n",
    "        #make copy of inputs\n",
    "        self.inputs= inputs\n",
    "        \n",
    "        #Calculate output values from inputs,weights and biases\n",
    "        self.output= np.dot(inputs ,self.weights)\n",
    "    #back pass\n",
    "    def Bpropogation(self,dval):\n",
    "        #gradient on parameters and values\n",
    "        self.dweights= np.dot(self.inputs.T,dval)\n",
    "        self.dbiases = np.sum(dval,axis=0,keepdims=True)\n",
    "        self.dinputs =np.dot(dval,self.weights.T)\n",
    "\n",
    "def confidenceTolist(x):\n",
    "    #to convert a given confidence matrix to list of predictions\n",
    "    rows = x.shape[0]\n",
    "    cols = x.shape[1]\n",
    "    predict=[None]*rows\n",
    "    for i in range(0,rows):\n",
    "        if(x[i,0]>x[i,1]):\n",
    "            predict[i]=0\n",
    "        else:\n",
    "            predict[i]=1\n",
    "    return predict\n",
    "        \n",
    "        \n",
    "    \n",
    "#Activation class with different activation func        \n",
    "class activation:\n",
    "    \n",
    "    #RELU activation\n",
    "    #makes copy of input values\n",
    "    def fRelu(self,x):\n",
    "        self.inputs=x\n",
    "        #calculates and stores the output\n",
    "        self.output = np.maximum(0,x)\n",
    "        #sigmoid\n",
    "        #self.inputs =x\n",
    "        #self.output = 1/(1+np.exp(-x))\n",
    "        #self.output = np.tanh(x) #tanh\n",
    "   \n",
    "    #sigmoid activation\n",
    "    def sigmoid(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = 1/(1+np.exp(-x))\n",
    "    \n",
    "    #tanh activation\n",
    "    def tanh(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = np.tanh(x)\n",
    "    \n",
    "    #for back pass    \n",
    "    def back(self,dval):\n",
    "        #copy of back pass inputs\n",
    "        self.dinputs = dval.copy()\n",
    "        \n",
    "        #Gradient to be set ) for -ve values\n",
    "        self.dinputs[self.inputs <=0] =0 \n",
    "        \n",
    "#Softmax activation\n",
    "class act_softmax :       \n",
    "    \n",
    "    #forward pass\n",
    "    def fsoftmax(self,inputs):\n",
    "        #make copy of input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #unnormalised probabities\n",
    "        exp1 = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        #normalising \n",
    "        prob = exp1 /np.sum(exp1 ,axis=1, keepdims=True)\n",
    "        \n",
    "        #storing and calculating output\n",
    "        self.output = prob\n",
    "    \n",
    "    #Back pass\n",
    "    def bsoftmax(self,dval):\n",
    "        \n",
    "        #create empty array\n",
    "        self.dinputs = np.empty_like(dval)\n",
    "        \n",
    "        #enumerate output and gradients\n",
    "        for i ,(single_o,single_dval) in enumerate(zip(self.output,dval)):\n",
    "            \n",
    "            #reshape output array\n",
    "            single_o = single_o.reshape(-1,1)\n",
    "            \n",
    "            #Calculate the jacobian matrix of the output\n",
    "            jacobian = np.diagflat(single_o)-np.dot(single_o,single_o.T)\n",
    "            \n",
    "            \n",
    "            self.dinputs[i] = np.dot(jacobian,single_dval)\n",
    "\n",
    "#Loss class             \n",
    "class loss:\n",
    "    \n",
    "    #for given model output and ground truth values calculate loss\n",
    "    def cal(self,output,y):\n",
    "        \n",
    "        sampleLosses = self.check(output,y)\n",
    "        \n",
    "        #average loss \n",
    "        data_loss= np.mean(sampleLosses)\n",
    "        \n",
    "        #return the loss\n",
    "        return data_loss\n",
    "\n",
    "#Cross Entropy Loss  inheriting the loss class  \n",
    "class CCE(loss):\n",
    "    #for forward pass\n",
    "    def check(self,y_pred,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x = len(y_pred)\n",
    "        \n",
    "        #clip data to prevent divison by 0\n",
    "        y_pred_clipped = np.clip(y_pred ,1e-7,1 - 1e-7)\n",
    "        \n",
    "        \n",
    "        #probablities for target values\n",
    "        confidence = y_pred_clipped[range(x),y_true]\n",
    "        negLikely = -np.log(confidence)\n",
    "        \n",
    "        #Return losses\n",
    "        return negLikely\n",
    "    \n",
    "    #for back pass\n",
    "    def backward(self,dval,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x=len(dval)\n",
    "        \n",
    "        # Number of labels in every sample\n",
    "        # use the first sample to count them\n",
    "        \n",
    "        label=len(dval[0])\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(label)[y_true] #\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs =-y_true/dval\n",
    "        # Normalize gradient\n",
    "        self.dinputs =self.dinputs/x\n",
    "       \n",
    "    \n",
    "class optimizer: #SGD\n",
    "    \n",
    "    #initialize optimizer \n",
    "    #set default settings\n",
    "    def __init__(self , l_rate=1.,decay=0.,p=0.):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.p =p\n",
    "    \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass \n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "    \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.p:\n",
    "            \n",
    "            #if the layer doesnt have momentum atrribute create them filled with 0\n",
    "            \n",
    "            if not hasattr(layer,'wieght_momentums'):\n",
    "                layer.weight_p = np.zeros_like(layer.weights)\n",
    "                layer.bias_p = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            \n",
    "            w_updates= self.p*layer.weight_p - self_curr_l_rate*layer.dweights\n",
    "            layer.weight_p = w_updates\n",
    "            \n",
    "            \n",
    "            # Build bias updates\n",
    "            b_updates= self.p*layer.bias_p - self_curr_l_rate*layer.dbiases\n",
    "            layer.bias_p = b_updates\n",
    "        \n",
    "        # SGD updates (as before momentum update)\n",
    "        else:\n",
    "            w_updates =-self.curr_l_rate * layer.dweights\n",
    "            b_updates =-self.curr_l_rate * layer.dbiases\n",
    "            \n",
    "        # Update weights and biases using either\n",
    "        #  momentum updates\n",
    "        \n",
    "        layer.weights+=w_updates\n",
    "        layer.biases+=b_updates\n",
    "    \n",
    "    #update iterations after every update\n",
    "    def post_update_params(self):\n",
    "        self.iter+=1\n",
    "\n",
    "# Adam optimizer        \n",
    "class  Adam_op:\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self , l_rate=.001,decay=0.,ep=1e-7,b1=0.9,b2=0.999):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.ep =ep\n",
    "        self.b1=b1\n",
    "        self.b2=b2\n",
    "    \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass\n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "            \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer,'w_cache'):\n",
    "            layer.w_p =np.zeros_like(layer.weights)\n",
    "            layer.w_cache=np.zeros_like(layer.weights)\n",
    "            layer.bias_p=np.zeros_like(layer.biases)\n",
    "            layer.bias_cache=np.zeros_like(layer.biases)\n",
    "            \n",
    "        #Update momentum with current gradients\n",
    "        layer.w_p = self.b1*layer.w_p + (1-self.b1)*layer.dweights\n",
    "        layer.bias_p=self.b1*layer.bias_p + (1-self.b1)*layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        w_p_corrected = layer.w_p/(1-self.b1**(self.iter +1))\n",
    "        bias_p_corrected = layer.bias_p/(1-self.b1**(self.iter +1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.w_cache = self.b2 * layer.w_cache + (1-self.b2)*layer.dweights**2\n",
    "        layer.bias_cache = self.b2*layer.bias_cache+(1-self.b2)*layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        w_cache_corrected = layer.w_cache/(1-self.b2)**(self.iter+1)\n",
    "        bias_cache_corrected = layer.bias_cache/(1-self.b2)**(self.iter+1)\n",
    "        \n",
    "        \n",
    "        # SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        \n",
    "        layer.weights+=-self.curr_l_rate*w_p_corrected/(np.sqrt(w_cache_corrected)+self.ep)\n",
    "        layer.biases+=-self.curr_l_rate*bias_p_corrected/(np.sqrt(bias_cache_corrected)+self.ep)\n",
    "    \n",
    "    #update iterations after every update    \n",
    "    def post_update_params(self):\n",
    "        self.iter+=1\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step        \n",
    "        \n",
    "class Activation_softmax_LCCE():\n",
    "    \n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation=act_softmax()\n",
    "        self.loss=CCE()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self,inputs,y_true):\n",
    "        \n",
    "        # Output layer's activation function\n",
    "        self.activation.fsoftmax(inputs)\n",
    "        \n",
    "        #store the output\n",
    "        self.output=self.activation.output\n",
    "        \n",
    "        #Calculate and return loss value\n",
    "        return self.loss.cal(self.output,y_true)\n",
    "    \n",
    "    # Backward pass    \n",
    "    def backpass(self,dval,y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        x=len(dval)\n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs =dval.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(x),y_true]-=1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs= self.dinputs/x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ,accuracy:0.742 ,loss:0.7447905083531015 ,lr:0.05,\n",
      "epoch:100 ,accuracy:0.774 ,loss:0.5239543559468138 ,lr:0.04999752512250644,\n",
      "epoch:200 ,accuracy:0.758 ,loss:0.5166351849101865 ,lr:0.04999502549496326,\n",
      "epoch:300 ,accuracy:0.742 ,loss:0.5417555170206504 ,lr:0.049992526117345455,\n",
      "epoch:400 ,accuracy:0.742 ,loss:0.5206480432822728 ,lr:0.04999002698961558,\n",
      "epoch:500 ,accuracy:0.742 ,loss:0.5150306575816576 ,lr:0.049987528111736124,\n",
      "epoch:600 ,accuracy:0.742 ,loss:0.520120660701345 ,lr:0.049985029483669646,\n",
      "epoch:700 ,accuracy:0.742 ,loss:0.5182203358794077 ,lr:0.049982531105378675,\n",
      "epoch:800 ,accuracy:0.742 ,loss:0.5173470114001129 ,lr:0.04998003297682575,\n",
      "epoch:900 ,accuracy:0.742 ,loss:0.5170907191652694 ,lr:0.049977535097973466,\n",
      "epoch:1000 ,accuracy:0.742 ,loss:0.5266011470534242 ,lr:0.049975037468784345,\n",
      "epoch:1100 ,accuracy:0.742 ,loss:0.5290970950399508 ,lr:0.049972540089220974,\n",
      "epoch:1200 ,accuracy:0.742 ,loss:0.521504948013835 ,lr:0.04997004295924593,\n",
      "epoch:1300 ,accuracy:0.742 ,loss:0.5224357265090134 ,lr:0.04996754607882181,\n",
      "epoch:1400 ,accuracy:0.742 ,loss:0.5191533796585287 ,lr:0.049965049447911185,\n",
      "epoch:1500 ,accuracy:0.742 ,loss:0.5187459213301753 ,lr:0.04996255306647668,\n",
      "epoch:1600 ,accuracy:0.742 ,loss:0.5181515794940982 ,lr:0.049960056934480884,\n",
      "epoch:1700 ,accuracy:0.742 ,loss:0.5178716355497046 ,lr:0.04995756105188642,\n",
      "epoch:1800 ,accuracy:0.742 ,loss:0.520413845221665 ,lr:0.049955065418655915,\n",
      "epoch:1900 ,accuracy:0.742 ,loss:0.5226230986732959 ,lr:0.04995257003475201,\n",
      "epoch:2000 ,accuracy:0.742 ,loss:0.5183096876940193 ,lr:0.04995007490013731,\n",
      "epoch:2100 ,accuracy:0.742 ,loss:0.5232824695998053 ,lr:0.0499475800147745,\n",
      "epoch:2200 ,accuracy:0.742 ,loss:0.516089668297427 ,lr:0.0499450853786262,\n",
      "epoch:2300 ,accuracy:0.742 ,loss:0.5166658020127353 ,lr:0.0499425909916551,\n",
      "epoch:2400 ,accuracy:0.742 ,loss:0.5202331516612346 ,lr:0.04994009685382384,\n",
      "epoch:2500 ,accuracy:0.742 ,loss:0.5152602367538895 ,lr:0.04993760296509512,\n",
      "epoch:2600 ,accuracy:0.742 ,loss:0.5158507390390901 ,lr:0.049935109325431604,\n",
      "epoch:2700 ,accuracy:0.742 ,loss:0.5153788565576075 ,lr:0.049932615934796004,\n",
      "epoch:2800 ,accuracy:0.742 ,loss:0.5148717038751127 ,lr:0.04993012279315098,\n",
      "epoch:2900 ,accuracy:0.742 ,loss:0.515004246873116 ,lr:0.049927629900459285,\n",
      "epoch:3000 ,accuracy:0.742 ,loss:0.5153843100606551 ,lr:0.049925137256683606,\n",
      "epoch:3100 ,accuracy:0.742 ,loss:0.5146748944463737 ,lr:0.04992264486178666,\n",
      "epoch:3200 ,accuracy:0.742 ,loss:0.5179503480020696 ,lr:0.04992015271573119,\n",
      "epoch:3300 ,accuracy:0.742 ,loss:0.5147870185435679 ,lr:0.04991766081847992,\n",
      "epoch:3400 ,accuracy:0.742 ,loss:0.5139031173663979 ,lr:0.049915169169995596,\n",
      "epoch:3500 ,accuracy:0.742 ,loss:0.5131221673588153 ,lr:0.049912677770240964,\n",
      "epoch:3600 ,accuracy:0.742 ,loss:0.513060261475554 ,lr:0.049910186619178794,\n",
      "epoch:3700 ,accuracy:0.742 ,loss:0.5169266000183368 ,lr:0.04990769571677183,\n",
      "epoch:3800 ,accuracy:0.742 ,loss:0.5256607674565527 ,lr:0.04990520506298287,\n",
      "epoch:3900 ,accuracy:0.742 ,loss:0.5147698302568272 ,lr:0.04990271465777467,\n",
      "epoch:4000 ,accuracy:0.742 ,loss:0.5084980024466625 ,lr:0.049900224501110035,\n",
      "epoch:4100 ,accuracy:0.742 ,loss:0.5163440446365661 ,lr:0.04989773459295174,\n",
      "epoch:4200 ,accuracy:0.742 ,loss:0.512638713520117 ,lr:0.04989524493326262,\n",
      "epoch:4300 ,accuracy:0.742 ,loss:0.517968119610585 ,lr:0.04989275552200545,\n",
      "epoch:4400 ,accuracy:0.742 ,loss:0.5176468544516596 ,lr:0.04989026635914307,\n",
      "epoch:4500 ,accuracy:0.742 ,loss:0.5180800345442568 ,lr:0.04988777744463829,\n",
      "epoch:4600 ,accuracy:0.742 ,loss:0.5126641358427272 ,lr:0.049885288778453954,\n",
      "epoch:4700 ,accuracy:0.742 ,loss:0.5149813548560922 ,lr:0.049882800360552884,\n",
      "epoch:4800 ,accuracy:0.742 ,loss:0.5122637411243888 ,lr:0.04988031219089794,\n",
      "epoch:4900 ,accuracy:0.742 ,loss:0.5116390867761749 ,lr:0.049877824269451976,\n",
      "epoch:5000 ,accuracy:0.742 ,loss:0.5158277630807154 ,lr:0.04987533659617785,\n",
      "epoch:5100 ,accuracy:0.742 ,loss:0.5038537839137095 ,lr:0.04987284917103844,\n",
      "epoch:5200 ,accuracy:0.742 ,loss:0.49460548895355944 ,lr:0.04987036199399661,\n",
      "epoch:5300 ,accuracy:0.742 ,loss:0.5222893371045695 ,lr:0.04986787506501525,\n",
      "epoch:5400 ,accuracy:0.742 ,loss:0.5099170444641832 ,lr:0.04986538838405724,\n",
      "epoch:5500 ,accuracy:0.742 ,loss:0.5242966743095422 ,lr:0.049862901951085496,\n",
      "epoch:5600 ,accuracy:0.742 ,loss:0.526272626166135 ,lr:0.049860415766062906,\n",
      "epoch:5700 ,accuracy:0.742 ,loss:0.510624001428517 ,lr:0.0498579298289524,\n",
      "epoch:5800 ,accuracy:0.742 ,loss:0.5141023743370253 ,lr:0.04985544413971689,\n",
      "epoch:5900 ,accuracy:0.742 ,loss:0.5102788005034715 ,lr:0.049852958698319315,\n",
      "epoch:6000 ,accuracy:0.742 ,loss:0.5179486901154309 ,lr:0.04985047350472258,\n",
      "epoch:6100 ,accuracy:0.742 ,loss:0.519060412937403 ,lr:0.04984798855888967,\n",
      "epoch:6200 ,accuracy:0.742 ,loss:0.5091227512193278 ,lr:0.049845503860783506,\n",
      "epoch:6300 ,accuracy:0.742 ,loss:0.5097970208914877 ,lr:0.049843019410367055,\n",
      "epoch:6400 ,accuracy:0.742 ,loss:0.5114215712341935 ,lr:0.04984053520760327,\n",
      "epoch:6500 ,accuracy:0.742 ,loss:0.5157295063843675 ,lr:0.049838051252455155,\n",
      "epoch:6600 ,accuracy:0.742 ,loss:0.5112398323680835 ,lr:0.049835567544885655,\n",
      "epoch:6700 ,accuracy:0.742 ,loss:0.5092150855590892 ,lr:0.04983308408485778,\n",
      "epoch:6800 ,accuracy:0.742 ,loss:0.5039240028073728 ,lr:0.0498306008723345,\n",
      "epoch:6900 ,accuracy:0.742 ,loss:0.5108990137536186 ,lr:0.04982811790727884,\n",
      "epoch:7000 ,accuracy:0.742 ,loss:0.5130406366412453 ,lr:0.04982563518965381,\n",
      "epoch:7100 ,accuracy:0.742 ,loss:0.5124370287342738 ,lr:0.049823152719422406,\n",
      "epoch:7200 ,accuracy:0.742 ,loss:0.5089260805339332 ,lr:0.049820670496547675,\n",
      "epoch:7300 ,accuracy:0.742 ,loss:0.5125773311395136 ,lr:0.04981818852099264,\n",
      "epoch:7400 ,accuracy:0.742 ,loss:0.5162420126182703 ,lr:0.049815706792720335,\n",
      "epoch:7500 ,accuracy:0.742 ,loss:0.5121705977414196 ,lr:0.0498132253116938,\n",
      "epoch:7600 ,accuracy:0.742 ,loss:0.5160939729108459 ,lr:0.04981074407787611,\n",
      "epoch:7700 ,accuracy:0.742 ,loss:0.5106754606983777 ,lr:0.049808263091230306,\n",
      "epoch:7800 ,accuracy:0.742 ,loss:0.5023116710781392 ,lr:0.04980578235171948,\n",
      "epoch:7900 ,accuracy:0.758 ,loss:0.5025654901170256 ,lr:0.04980330185930667,\n",
      "epoch:8000 ,accuracy:0.774 ,loss:0.5106318435381978 ,lr:0.04980082161395499,\n",
      "epoch:8100 ,accuracy:0.742 ,loss:0.4964731266914132 ,lr:0.04979834161562752,\n",
      "epoch:8200 ,accuracy:0.742 ,loss:0.5135620673389392 ,lr:0.04979586186428736,\n",
      "epoch:8300 ,accuracy:0.742 ,loss:0.5070289130013175 ,lr:0.04979338235989761,\n",
      "epoch:8400 ,accuracy:0.774 ,loss:0.5149662543936472 ,lr:0.04979090310242139,\n",
      "epoch:8500 ,accuracy:0.758 ,loss:0.5176805307464373 ,lr:0.049788424091821805,\n",
      "epoch:8600 ,accuracy:0.758 ,loss:0.5061718595742654 ,lr:0.049785945328062006,\n",
      "epoch:8700 ,accuracy:0.742 ,loss:0.516166454196699 ,lr:0.0497834668111051,\n",
      "epoch:8800 ,accuracy:0.742 ,loss:0.493165509471432 ,lr:0.049780988540914256,\n",
      "epoch:8900 ,accuracy:0.758 ,loss:0.48755465273034665 ,lr:0.0497785105174526,\n",
      "epoch:9000 ,accuracy:0.742 ,loss:0.5013414482967938 ,lr:0.04977603274068329,\n",
      "epoch:9100 ,accuracy:0.742 ,loss:0.5086610633468504 ,lr:0.04977355521056952,\n",
      "epoch:9200 ,accuracy:0.742 ,loss:0.49733975314308104 ,lr:0.049771077927074414,\n",
      "epoch:9300 ,accuracy:0.758 ,loss:0.4881769105543048 ,lr:0.0497686008901612,\n",
      "epoch:9400 ,accuracy:0.758 ,loss:0.49860297749642657 ,lr:0.04976612409979302,\n",
      "epoch:9500 ,accuracy:0.758 ,loss:0.48972631474990286 ,lr:0.0497636475559331,\n",
      "epoch:9600 ,accuracy:0.774 ,loss:0.502068082456615 ,lr:0.049761171258544616,\n",
      "epoch:9700 ,accuracy:0.758 ,loss:0.5001860617861343 ,lr:0.0497586952075908,\n",
      "epoch:9800 ,accuracy:0.758 ,loss:0.4924220727031692 ,lr:0.04975621940303483,\n",
      "epoch:9900 ,accuracy:0.758 ,loss:0.4923200967539796 ,lr:0.049753743844839965,\n",
      "epoch:10000 ,accuracy:0.758 ,loss:0.49233705868491584 ,lr:0.04975126853296942,\n",
      "epoch:10100 ,accuracy:0.758 ,loss:0.48485914571925237 ,lr:0.04974879346738644,\n",
      "epoch:10200 ,accuracy:0.758 ,loss:0.49040230122984163 ,lr:0.04974631864805425,\n",
      "epoch:10300 ,accuracy:0.774 ,loss:0.49773082204129115 ,lr:0.04974384407493612,\n",
      "epoch:10400 ,accuracy:0.774 ,loss:0.5285397396745443 ,lr:0.0497413697479953,\n",
      "epoch:10500 ,accuracy:0.758 ,loss:0.4889037959848737 ,lr:0.04973889566719507,\n",
      "epoch:10600 ,accuracy:0.758 ,loss:0.48332762549521113 ,lr:0.04973642183249868,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10700 ,accuracy:0.774 ,loss:0.5142655439015246 ,lr:0.049733948243869425,\n",
      "epoch:10800 ,accuracy:0.758 ,loss:0.49238080157328107 ,lr:0.04973147490127059,\n",
      "epoch:10900 ,accuracy:0.758 ,loss:0.49032463958491157 ,lr:0.04972900180466547,\n",
      "epoch:11000 ,accuracy:0.758 ,loss:0.49018864122029077 ,lr:0.049726528954017385,\n"
     ]
    }
   ],
   "source": [
    "input2=data.loc[:,['Age', 'Weight', 'Delivery phase', 'HB', 'IFA', 'BP', 'Residence']]\n",
    "result=data['Result']\n",
    "\n",
    "#Performing test train split\n",
    "input1,x_test, result1, y_test = train_test_split(input2, data['Result'], test_size=0.35, random_state=0)\n",
    "\n",
    "#creating first model\n",
    "model1=NN(numHlayers=1)\n",
    "model1.fit(input1,result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.853 ,lr:0.04972650422675286,\n"
     ]
    }
   ],
   "source": [
    "#predict values using model 1\n",
    "y=model1.predict(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[ 3  0]\n",
      " [ 5 26]]\n",
      "\n",
      "\n",
      "Precision : 1.0\n",
      "Recall : 0.8387096774193549\n",
      "F1 SCORE : 0.9122807017543859\n"
     ]
    }
   ],
   "source": [
    "model1.CM(y_test.tolist(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ,accuracy:0.258 ,loss:2.778489961161019 ,lr:0.05,\n",
      "epoch:100 ,accuracy:0.742 ,loss:0.5351005729928315 ,lr:0.04999752512250644,\n",
      "epoch:200 ,accuracy:0.758 ,loss:0.542391976620342 ,lr:0.04999502549496326,\n",
      "epoch:300 ,accuracy:0.742 ,loss:0.5243466758835301 ,lr:0.049992526117345455,\n",
      "epoch:400 ,accuracy:0.742 ,loss:0.5291152792710466 ,lr:0.04999002698961558,\n",
      "epoch:500 ,accuracy:0.742 ,loss:0.525159749676431 ,lr:0.049987528111736124,\n",
      "epoch:600 ,accuracy:0.742 ,loss:0.5230158539423045 ,lr:0.049985029483669646,\n",
      "epoch:700 ,accuracy:0.742 ,loss:0.5252456946354428 ,lr:0.049982531105378675,\n",
      "epoch:800 ,accuracy:0.742 ,loss:0.5256125049094207 ,lr:0.04998003297682575,\n",
      "epoch:900 ,accuracy:0.742 ,loss:0.5256951816421523 ,lr:0.049977535097973466,\n",
      "epoch:1000 ,accuracy:0.742 ,loss:0.5218899764083991 ,lr:0.049975037468784345,\n",
      "epoch:1100 ,accuracy:0.742 ,loss:0.5204481978822335 ,lr:0.049972540089220974,\n",
      "epoch:1200 ,accuracy:0.758 ,loss:0.5242782000720241 ,lr:0.04997004295924593,\n",
      "epoch:1300 ,accuracy:0.742 ,loss:0.5181589329605456 ,lr:0.04996754607882181,\n",
      "epoch:1400 ,accuracy:0.758 ,loss:0.5208460774473685 ,lr:0.049965049447911185,\n",
      "epoch:1500 ,accuracy:0.758 ,loss:0.521190066603859 ,lr:0.04996255306647668,\n",
      "epoch:1600 ,accuracy:0.758 ,loss:0.5215761039245151 ,lr:0.049960056934480884,\n",
      "epoch:1700 ,accuracy:0.758 ,loss:0.5239416180657724 ,lr:0.04995756105188642,\n",
      "epoch:1800 ,accuracy:0.758 ,loss:0.5227070116179461 ,lr:0.049955065418655915,\n",
      "epoch:1900 ,accuracy:0.758 ,loss:0.5244984925075725 ,lr:0.04995257003475201,\n",
      "epoch:2000 ,accuracy:0.774 ,loss:0.5223358972345145 ,lr:0.04995007490013731,\n",
      "epoch:2100 ,accuracy:0.758 ,loss:0.5208690100402107 ,lr:0.0499475800147745,\n",
      "epoch:2200 ,accuracy:0.758 ,loss:0.5221181584542819 ,lr:0.0499450853786262,\n",
      "epoch:2300 ,accuracy:0.758 ,loss:0.5222028752441435 ,lr:0.0499425909916551,\n",
      "epoch:2400 ,accuracy:0.774 ,loss:0.5220466379301402 ,lr:0.04994009685382384,\n",
      "epoch:2500 ,accuracy:0.758 ,loss:0.5203344682360447 ,lr:0.04993760296509512,\n",
      "epoch:2600 ,accuracy:0.758 ,loss:0.5213830811502154 ,lr:0.049935109325431604,\n",
      "epoch:2700 ,accuracy:0.774 ,loss:0.5213680066708354 ,lr:0.049932615934796004,\n",
      "epoch:2800 ,accuracy:0.774 ,loss:0.521331782186451 ,lr:0.04993012279315098,\n",
      "epoch:2900 ,accuracy:0.774 ,loss:0.5220130495011581 ,lr:0.049927629900459285,\n",
      "epoch:3000 ,accuracy:0.774 ,loss:0.5216649656417176 ,lr:0.049925137256683606,\n",
      "epoch:3100 ,accuracy:0.758 ,loss:0.5220663456524597 ,lr:0.04992264486178666,\n",
      "epoch:3200 ,accuracy:0.774 ,loss:0.5226727012800169 ,lr:0.04992015271573119,\n",
      "epoch:3300 ,accuracy:0.790 ,loss:0.5233834453112585 ,lr:0.04991766081847992,\n",
      "epoch:3400 ,accuracy:0.774 ,loss:0.5222506038864174 ,lr:0.049915169169995596,\n",
      "epoch:3500 ,accuracy:0.758 ,loss:0.5212937580956728 ,lr:0.049912677770240964,\n",
      "epoch:3600 ,accuracy:0.758 ,loss:0.5209304409020612 ,lr:0.049910186619178794,\n",
      "epoch:3700 ,accuracy:0.758 ,loss:0.5230355663267043 ,lr:0.04990769571677183,\n",
      "epoch:3800 ,accuracy:0.758 ,loss:0.5224288806481926 ,lr:0.04990520506298287,\n",
      "epoch:3900 ,accuracy:0.774 ,loss:0.5204349023197836 ,lr:0.04990271465777467,\n",
      "epoch:4000 ,accuracy:0.742 ,loss:0.5873165056227119 ,lr:0.049900224501110035,\n",
      "epoch:4100 ,accuracy:0.758 ,loss:0.5111824192388859 ,lr:0.04989773459295174,\n",
      "epoch:4200 ,accuracy:0.758 ,loss:0.5103187384547848 ,lr:0.04989524493326262,\n",
      "epoch:4300 ,accuracy:0.758 ,loss:0.5114408317936717 ,lr:0.04989275552200545,\n",
      "epoch:4400 ,accuracy:0.758 ,loss:0.5113363150164281 ,lr:0.04989026635914307,\n",
      "epoch:4500 ,accuracy:0.758 ,loss:0.5101318557359618 ,lr:0.04988777744463829,\n",
      "epoch:4600 ,accuracy:0.758 ,loss:0.5099032891838545 ,lr:0.049885288778453954,\n",
      "epoch:4700 ,accuracy:0.758 ,loss:0.5077155629095543 ,lr:0.049882800360552884,\n",
      "epoch:4800 ,accuracy:0.758 ,loss:0.5094166957694453 ,lr:0.04988031219089794,\n",
      "epoch:4900 ,accuracy:0.758 ,loss:0.5092745729151821 ,lr:0.049877824269451976,\n",
      "epoch:5000 ,accuracy:0.758 ,loss:0.5066254716585776 ,lr:0.04987533659617785,\n",
      "epoch:5100 ,accuracy:0.758 ,loss:0.5076197959740874 ,lr:0.04987284917103844,\n",
      "epoch:5200 ,accuracy:0.774 ,loss:0.5103720259910461 ,lr:0.04987036199399661,\n",
      "epoch:5300 ,accuracy:0.774 ,loss:0.5132267095919423 ,lr:0.04986787506501525,\n",
      "epoch:5400 ,accuracy:0.758 ,loss:0.5063697232980425 ,lr:0.04986538838405724,\n",
      "epoch:5500 ,accuracy:0.758 ,loss:0.5064237997118013 ,lr:0.049862901951085496,\n",
      "epoch:5600 ,accuracy:0.774 ,loss:0.5140550957543154 ,lr:0.049860415766062906,\n",
      "epoch:5700 ,accuracy:0.742 ,loss:0.5030837115968506 ,lr:0.0498579298289524,\n",
      "epoch:5800 ,accuracy:0.742 ,loss:0.5134663825346637 ,lr:0.04985544413971689,\n",
      "epoch:5900 ,accuracy:0.742 ,loss:0.518176314303861 ,lr:0.049852958698319315,\n",
      "epoch:6000 ,accuracy:0.742 ,loss:0.5063190601570124 ,lr:0.04985047350472258,\n",
      "epoch:6100 ,accuracy:0.742 ,loss:0.5035637213702178 ,lr:0.04984798855888967,\n",
      "epoch:6200 ,accuracy:0.742 ,loss:0.5083594331784788 ,lr:0.049845503860783506,\n",
      "epoch:6300 ,accuracy:0.742 ,loss:0.4988066855717122 ,lr:0.049843019410367055,\n",
      "epoch:6400 ,accuracy:0.742 ,loss:0.5020906722459128 ,lr:0.04984053520760327,\n",
      "epoch:6500 ,accuracy:0.758 ,loss:0.4973425121758666 ,lr:0.049838051252455155,\n",
      "epoch:6600 ,accuracy:0.742 ,loss:0.490532621280594 ,lr:0.049835567544885655,\n",
      "epoch:6700 ,accuracy:0.742 ,loss:0.4966332391881207 ,lr:0.04983308408485778,\n",
      "epoch:6800 ,accuracy:0.742 ,loss:0.5141433667868485 ,lr:0.0498306008723345,\n",
      "epoch:6900 ,accuracy:0.742 ,loss:0.5103268317818793 ,lr:0.04982811790727884,\n",
      "epoch:7000 ,accuracy:0.742 ,loss:0.4996047309721731 ,lr:0.04982563518965381,\n",
      "epoch:7100 ,accuracy:0.742 ,loss:0.49907033510337917 ,lr:0.049823152719422406,\n",
      "epoch:7200 ,accuracy:0.742 ,loss:0.49307016794651903 ,lr:0.049820670496547675,\n",
      "epoch:7300 ,accuracy:0.742 ,loss:0.4949540662967687 ,lr:0.04981818852099264,\n",
      "epoch:7400 ,accuracy:0.742 ,loss:0.4856410730441397 ,lr:0.049815706792720335,\n",
      "epoch:7500 ,accuracy:0.742 ,loss:0.48858658827325463 ,lr:0.0498132253116938,\n",
      "epoch:7600 ,accuracy:0.758 ,loss:0.4800183302475466 ,lr:0.04981074407787611,\n",
      "epoch:7700 ,accuracy:0.774 ,loss:0.4972301701620726 ,lr:0.049808263091230306,\n",
      "epoch:7800 ,accuracy:0.774 ,loss:0.48838877872434433 ,lr:0.04980578235171948,\n",
      "epoch:7900 ,accuracy:0.774 ,loss:0.48837421724112273 ,lr:0.04980330185930667,\n",
      "epoch:8000 ,accuracy:0.774 ,loss:0.4883600477103277 ,lr:0.04980082161395499,\n",
      "epoch:8100 ,accuracy:0.774 ,loss:0.4902827470061083 ,lr:0.04979834161562752,\n",
      "epoch:8200 ,accuracy:0.774 ,loss:0.4822992024874559 ,lr:0.04979586186428736,\n",
      "epoch:8300 ,accuracy:0.774 ,loss:0.48601833725368637 ,lr:0.04979338235989761,\n",
      "epoch:8400 ,accuracy:0.774 ,loss:0.4751348678646684 ,lr:0.04979090310242139,\n",
      "epoch:8500 ,accuracy:0.774 ,loss:0.4811112610226848 ,lr:0.049788424091821805,\n",
      "epoch:8600 ,accuracy:0.774 ,loss:0.48710012899187866 ,lr:0.049785945328062006,\n",
      "epoch:8700 ,accuracy:0.774 ,loss:0.48963945026827055 ,lr:0.0497834668111051,\n",
      "epoch:8800 ,accuracy:0.774 ,loss:0.4883152074206988 ,lr:0.049780988540914256,\n",
      "epoch:8900 ,accuracy:0.742 ,loss:0.4882388679935838 ,lr:0.0497785105174526,\n",
      "epoch:9000 ,accuracy:0.758 ,loss:0.4833318245016094 ,lr:0.04977603274068329,\n",
      "epoch:9100 ,accuracy:0.758 ,loss:0.473764591831137 ,lr:0.04977355521056952,\n",
      "epoch:9200 ,accuracy:0.774 ,loss:0.48629916361575504 ,lr:0.049771077927074414,\n",
      "epoch:9300 ,accuracy:0.774 ,loss:0.4861067791188964 ,lr:0.0497686008901612,\n",
      "epoch:9400 ,accuracy:0.774 ,loss:0.4818363628562382 ,lr:0.04976612409979302,\n",
      "epoch:9500 ,accuracy:0.758 ,loss:0.4794418916835917 ,lr:0.0497636475559331,\n",
      "epoch:9600 ,accuracy:0.758 ,loss:0.4880324983672728 ,lr:0.049761171258544616,\n",
      "epoch:9700 ,accuracy:0.758 ,loss:0.49747254517187206 ,lr:0.0497586952075908,\n",
      "epoch:9800 ,accuracy:0.758 ,loss:0.4786638919368673 ,lr:0.04975621940303483,\n",
      "epoch:9900 ,accuracy:0.758 ,loss:0.4785295494032147 ,lr:0.049753743844839965,\n",
      "epoch:10000 ,accuracy:0.758 ,loss:0.4940775728222615 ,lr:0.04975126853296942,\n",
      "epoch:10100 ,accuracy:0.774 ,loss:0.4822997550692818 ,lr:0.04974879346738644,\n",
      "epoch:10200 ,accuracy:0.758 ,loss:0.47565279567888036 ,lr:0.04974631864805425,\n",
      "epoch:10300 ,accuracy:0.758 ,loss:0.4839826873592661 ,lr:0.04974384407493612,\n",
      "epoch:10400 ,accuracy:0.758 ,loss:0.4684686933212079 ,lr:0.0497413697479953,\n",
      "epoch:10500 ,accuracy:0.758 ,loss:0.48319729896523367 ,lr:0.04973889566719507,\n",
      "epoch:10600 ,accuracy:0.758 ,loss:0.488732451609593 ,lr:0.04973642183249868,\n",
      "epoch:10700 ,accuracy:0.758 ,loss:0.4810370480695192 ,lr:0.049733948243869425,\n",
      "epoch:10800 ,accuracy:0.758 ,loss:0.47624060153042874 ,lr:0.04973147490127059,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10900 ,accuracy:0.758 ,loss:0.4868556284980015 ,lr:0.04972900180466547,\n",
      "epoch:11000 ,accuracy:0.758 ,loss:0.4721749890848062 ,lr:0.049726528954017385,\n"
     ]
    }
   ],
   "source": [
    "model2=NN(numHlayers=1)\n",
    "model2.fit(input1,result1)\n",
    "#training model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.765 ,lr:0.04972650422675286,\n"
     ]
    }
   ],
   "source": [
    "y=model2.predict(x_test,y_test)\n",
    "#testing model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[ 0  0]\n",
      " [ 8 26]]\n",
      "\n",
      "\n",
      "Precision : 1.0\n",
      "Recall : 0.7647058823529411\n",
      "F1 SCORE : 0.8666666666666666\n"
     ]
    }
   ],
   "source": [
    "model2.CM(y_test.tolist(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE Synthetic Minority Oversampling Technique like behaviour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data[(data['Result']==0)]\n",
    "data2.head(3)\n",
    "new_data=pd.concat([data,data2])\n",
    "#new_data=pd.concat([new_data,data2])\n",
    "new_data.head(3)\n",
    "new_data=new_data.sample(frac=1)\n",
    "new_data.head(3)\n",
    "new_input=new_data.loc[:,['Age', 'Weight', 'Delivery phase', 'HB', 'IFA', 'BP', 'Residence']]\n",
    "new_result=new_data['Result']\n",
    "input1,x_test, result1, y_test = train_test_split(new_input, new_result, test_size=0.33, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ,accuracy:0.550 ,loss:0.8850256014982048 ,lr:0.05,\n",
      "epoch:100 ,accuracy:0.650 ,loss:0.6245100929138115 ,lr:0.04999752512250644,\n",
      "epoch:200 ,accuracy:0.662 ,loss:0.6106375572067992 ,lr:0.04999502549496326,\n",
      "epoch:300 ,accuracy:0.662 ,loss:0.6049313942556622 ,lr:0.049992526117345455,\n",
      "epoch:400 ,accuracy:0.675 ,loss:0.5837776734387793 ,lr:0.04999002698961558,\n",
      "epoch:500 ,accuracy:0.662 ,loss:0.6042811588169344 ,lr:0.049987528111736124,\n",
      "epoch:600 ,accuracy:0.675 ,loss:0.5915195018224331 ,lr:0.049985029483669646,\n",
      "epoch:700 ,accuracy:0.600 ,loss:0.5932878292741585 ,lr:0.049982531105378675,\n",
      "epoch:800 ,accuracy:0.450 ,loss:0.640743395204362 ,lr:0.04998003297682575,\n",
      "epoch:900 ,accuracy:0.450 ,loss:0.6387380737357474 ,lr:0.049977535097973466,\n",
      "epoch:1000 ,accuracy:0.450 ,loss:0.6395730058639403 ,lr:0.049975037468784345,\n",
      "epoch:1100 ,accuracy:0.450 ,loss:0.638149601361453 ,lr:0.049972540089220974,\n",
      "epoch:1200 ,accuracy:0.450 ,loss:0.6374011405413754 ,lr:0.04997004295924593,\n",
      "epoch:1300 ,accuracy:0.450 ,loss:0.6372109129059623 ,lr:0.04996754607882181,\n",
      "epoch:1400 ,accuracy:0.450 ,loss:0.6370444095073919 ,lr:0.049965049447911185,\n",
      "epoch:1500 ,accuracy:0.450 ,loss:0.6369347773005296 ,lr:0.04996255306647668,\n",
      "epoch:1600 ,accuracy:0.450 ,loss:0.63601487518804 ,lr:0.049960056934480884,\n",
      "epoch:1700 ,accuracy:0.450 ,loss:0.6405020359693242 ,lr:0.04995756105188642,\n",
      "epoch:1800 ,accuracy:0.450 ,loss:0.6763575404331486 ,lr:0.049955065418655915,\n",
      "epoch:1900 ,accuracy:0.450 ,loss:0.6346958024219249 ,lr:0.04995257003475201,\n",
      "epoch:2000 ,accuracy:0.450 ,loss:0.6385420449007654 ,lr:0.04995007490013731,\n",
      "epoch:2100 ,accuracy:0.450 ,loss:0.6362383499984849 ,lr:0.0499475800147745,\n",
      "epoch:2200 ,accuracy:0.450 ,loss:0.6328906797285737 ,lr:0.0499450853786262,\n",
      "epoch:2300 ,accuracy:0.450 ,loss:0.6384940127946287 ,lr:0.0499425909916551,\n",
      "epoch:2400 ,accuracy:0.450 ,loss:0.6380069586706621 ,lr:0.04994009685382384,\n",
      "epoch:2500 ,accuracy:0.450 ,loss:0.6365118817375639 ,lr:0.04993760296509512,\n",
      "epoch:2600 ,accuracy:0.450 ,loss:0.6347086490978269 ,lr:0.049935109325431604,\n",
      "epoch:2700 ,accuracy:0.450 ,loss:0.6449721622200701 ,lr:0.049932615934796004,\n",
      "epoch:2800 ,accuracy:0.450 ,loss:0.6464663976219891 ,lr:0.04993012279315098,\n",
      "epoch:2900 ,accuracy:0.450 ,loss:0.6401563160210199 ,lr:0.049927629900459285,\n",
      "epoch:3000 ,accuracy:0.450 ,loss:0.6291099684488277 ,lr:0.049925137256683606,\n",
      "epoch:3100 ,accuracy:0.450 ,loss:0.6252988702709362 ,lr:0.04992264486178666,\n",
      "epoch:3200 ,accuracy:0.450 ,loss:0.6338560046421026 ,lr:0.04992015271573119,\n",
      "epoch:3300 ,accuracy:0.450 ,loss:0.6234501127237368 ,lr:0.04991766081847992,\n",
      "epoch:3400 ,accuracy:0.450 ,loss:0.6433632994049141 ,lr:0.049915169169995596,\n",
      "epoch:3500 ,accuracy:0.450 ,loss:0.6284982170324318 ,lr:0.049912677770240964,\n",
      "epoch:3600 ,accuracy:0.450 ,loss:0.6283617184550547 ,lr:0.049910186619178794,\n",
      "epoch:3700 ,accuracy:0.450 ,loss:0.6282496321872035 ,lr:0.04990769571677183,\n",
      "epoch:3800 ,accuracy:0.450 ,loss:0.6239202538532581 ,lr:0.04990520506298287,\n",
      "epoch:3900 ,accuracy:0.450 ,loss:0.6237509543048277 ,lr:0.04990271465777467,\n",
      "epoch:4000 ,accuracy:0.450 ,loss:0.6226197763757486 ,lr:0.049900224501110035,\n",
      "epoch:4100 ,accuracy:0.450 ,loss:0.6276729412500422 ,lr:0.04989773459295174,\n",
      "epoch:4200 ,accuracy:0.450 ,loss:0.6226252039015414 ,lr:0.04989524493326262,\n",
      "epoch:4300 ,accuracy:0.450 ,loss:0.6273886752310588 ,lr:0.04989275552200545,\n",
      "epoch:4400 ,accuracy:0.450 ,loss:0.6272709569080863 ,lr:0.04989026635914307,\n",
      "epoch:4500 ,accuracy:0.450 ,loss:0.6226183192174649 ,lr:0.04988777744463829,\n",
      "epoch:4600 ,accuracy:0.450 ,loss:0.6220836403190473 ,lr:0.049885288778453954,\n",
      "epoch:4700 ,accuracy:0.450 ,loss:0.6222704560762728 ,lr:0.049882800360552884,\n",
      "epoch:4800 ,accuracy:0.450 ,loss:0.6267409471023804 ,lr:0.04988031219089794,\n",
      "epoch:4900 ,accuracy:0.450 ,loss:0.6222140516839104 ,lr:0.049877824269451976,\n",
      "epoch:5000 ,accuracy:0.450 ,loss:0.6328005110233944 ,lr:0.04987533659617785,\n",
      "epoch:5100 ,accuracy:0.450 ,loss:0.6213938889214229 ,lr:0.04987284917103844,\n",
      "epoch:5200 ,accuracy:0.450 ,loss:0.6224331164589889 ,lr:0.04987036199399661,\n",
      "epoch:5300 ,accuracy:0.450 ,loss:0.6325378321626853 ,lr:0.04986787506501525,\n",
      "epoch:5400 ,accuracy:0.450 ,loss:0.6211660645785214 ,lr:0.04986538838405724,\n",
      "epoch:5500 ,accuracy:0.450 ,loss:0.6220049938288446 ,lr:0.049862901951085496,\n",
      "epoch:5600 ,accuracy:0.450 ,loss:0.6327388497544776 ,lr:0.049860415766062906,\n",
      "epoch:5700 ,accuracy:0.450 ,loss:0.6210295170165873 ,lr:0.0498579298289524,\n",
      "epoch:5800 ,accuracy:0.450 ,loss:0.6209928668544696 ,lr:0.04985544413971689,\n",
      "epoch:5900 ,accuracy:0.450 ,loss:0.6217711020004566 ,lr:0.049852958698319315,\n",
      "epoch:6000 ,accuracy:0.450 ,loss:0.6207511431977244 ,lr:0.04985047350472258,\n",
      "epoch:6100 ,accuracy:0.450 ,loss:0.6256564369798807 ,lr:0.04984798855888967,\n",
      "epoch:6200 ,accuracy:0.450 ,loss:0.6257099118164676 ,lr:0.049845503860783506,\n",
      "epoch:6300 ,accuracy:0.450 ,loss:0.621337669844243 ,lr:0.049843019410367055,\n",
      "epoch:6400 ,accuracy:0.450 ,loss:0.6210861248343219 ,lr:0.04984053520760327,\n",
      "epoch:6500 ,accuracy:0.450 ,loss:0.6212142309240709 ,lr:0.049838051252455155,\n",
      "epoch:6600 ,accuracy:0.450 ,loss:0.6206605258201163 ,lr:0.049835567544885655,\n",
      "epoch:6700 ,accuracy:0.450 ,loss:0.6210400411835588 ,lr:0.04983308408485778,\n",
      "epoch:6800 ,accuracy:0.450 ,loss:0.6211609777847178 ,lr:0.0498306008723345,\n",
      "epoch:6900 ,accuracy:0.450 ,loss:0.6205481085614266 ,lr:0.04982811790727884,\n",
      "epoch:7000 ,accuracy:0.450 ,loss:0.6208944018468284 ,lr:0.04982563518965381,\n",
      "epoch:7100 ,accuracy:0.450 ,loss:0.6209579732905489 ,lr:0.049823152719422406,\n",
      "epoch:7200 ,accuracy:0.450 ,loss:0.6210291543567843 ,lr:0.049820670496547675,\n",
      "epoch:7300 ,accuracy:0.450 ,loss:0.6204211971271041 ,lr:0.04981818852099264,\n",
      "epoch:7400 ,accuracy:0.450 ,loss:0.620724711144635 ,lr:0.049815706792720335,\n",
      "epoch:7500 ,accuracy:0.450 ,loss:0.6207755582725663 ,lr:0.0498132253116938,\n",
      "epoch:7600 ,accuracy:0.450 ,loss:0.6208269543754895 ,lr:0.04981074407787611,\n",
      "epoch:7700 ,accuracy:0.450 ,loss:0.6208582653320281 ,lr:0.049808263091230306,\n",
      "epoch:7800 ,accuracy:0.450 ,loss:0.6202751405318413 ,lr:0.04980578235171948,\n",
      "epoch:7900 ,accuracy:0.450 ,loss:0.6202093848672312 ,lr:0.04980330185930667,\n",
      "epoch:8000 ,accuracy:0.450 ,loss:0.6205472811859305 ,lr:0.04980082161395499,\n",
      "epoch:8100 ,accuracy:0.450 ,loss:0.6206077388838538 ,lr:0.04979834161562752,\n",
      "epoch:8200 ,accuracy:0.450 ,loss:0.620695870332648 ,lr:0.04979586186428736,\n",
      "epoch:8300 ,accuracy:0.450 ,loss:0.6201315437225772 ,lr:0.04979338235989761,\n",
      "epoch:8400 ,accuracy:0.450 ,loss:0.6200606387944149 ,lr:0.04979090310242139,\n",
      "epoch:8500 ,accuracy:0.450 ,loss:0.620396756821944 ,lr:0.049788424091821805,\n",
      "epoch:8600 ,accuracy:0.450 ,loss:0.6246962030987302 ,lr:0.049785945328062006,\n",
      "epoch:8700 ,accuracy:0.450 ,loss:0.6206097739931069 ,lr:0.0497834668111051,\n",
      "epoch:8800 ,accuracy:0.450 ,loss:0.6199545568392485 ,lr:0.049780988540914256,\n",
      "epoch:8900 ,accuracy:0.450 ,loss:0.6208326054059493 ,lr:0.0497785105174526,\n",
      "epoch:9000 ,accuracy:0.450 ,loss:0.6205216409512525 ,lr:0.04977603274068329,\n",
      "epoch:9100 ,accuracy:0.450 ,loss:0.620711680137273 ,lr:0.04977355521056952,\n",
      "epoch:9200 ,accuracy:0.450 ,loss:0.6196910824075477 ,lr:0.049771077927074414,\n",
      "epoch:9300 ,accuracy:0.450 ,loss:0.6204917784455012 ,lr:0.0497686008901612,\n",
      "epoch:9400 ,accuracy:0.450 ,loss:0.6198424224729392 ,lr:0.04976612409979302,\n",
      "epoch:9500 ,accuracy:0.450 ,loss:0.62041706661521 ,lr:0.0497636475559331,\n",
      "epoch:9600 ,accuracy:0.450 ,loss:0.6203841106590711 ,lr:0.049761171258544616,\n",
      "epoch:9700 ,accuracy:0.450 ,loss:0.6196810354690547 ,lr:0.0497586952075908,\n",
      "epoch:9800 ,accuracy:0.450 ,loss:0.6197329876046165 ,lr:0.04975621940303483,\n",
      "epoch:9900 ,accuracy:0.450 ,loss:0.6203101199272961 ,lr:0.049753743844839965,\n",
      "epoch:10000 ,accuracy:0.450 ,loss:0.6195948054876912 ,lr:0.04975126853296942,\n",
      "epoch:10100 ,accuracy:0.450 ,loss:0.6276892980840694 ,lr:0.04974879346738644,\n",
      "epoch:10200 ,accuracy:0.450 ,loss:0.6206010868837146 ,lr:0.04974631864805425,\n",
      "epoch:10300 ,accuracy:0.450 ,loss:0.6281366177492584 ,lr:0.04974384407493612,\n",
      "epoch:10400 ,accuracy:0.450 ,loss:0.6203719158529267 ,lr:0.0497413697479953,\n",
      "epoch:10500 ,accuracy:0.450 ,loss:0.6206745337597832 ,lr:0.04973889566719507,\n",
      "epoch:10600 ,accuracy:0.450 ,loss:0.6394908690264725 ,lr:0.04973642183249868,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10700 ,accuracy:0.450 ,loss:0.6194536815935554 ,lr:0.049733948243869425,\n",
      "epoch:10800 ,accuracy:0.450 ,loss:0.6194176871222634 ,lr:0.04973147490127059,\n",
      "epoch:10900 ,accuracy:0.450 ,loss:0.6194592831746141 ,lr:0.04972900180466547,\n",
      "epoch:11000 ,accuracy:0.450 ,loss:0.6193483403912161 ,lr:0.049726528954017385,\n"
     ]
    }
   ],
   "source": [
    "#creating testing and training model 3\n",
    "model3=NN(numHlayers=1)\n",
    "model3.fit(input1,result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.300 ,lr:0.04972650422675286,\n"
     ]
    }
   ],
   "source": [
    "y=model3.predict(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b4617a8c2ca2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-832d932fafa5>\u001b[0m in \u001b[0;36mCM\u001b[1;34m(self, y_test, y_test_obs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#precision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m         \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#recall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m         \u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "model3.CM(y_test.tolist(),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
