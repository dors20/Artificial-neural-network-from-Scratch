{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#To reset seed so same set of numbers are produced\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('LBW_Dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Community</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Delivery phase</th>\n",
       "      <th>HB</th>\n",
       "      <th>IFA</th>\n",
       "      <th>BP</th>\n",
       "      <th>Education</th>\n",
       "      <th>Residence</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.375</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.500</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.125</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.375</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Community   Age     Weight  Delivery phase   HB  IFA     BP  \\\n",
       "0           0          1  21.0  42.000000             1.0  9.2    1  1.375   \n",
       "1           1          1  21.0  45.129412             1.0  8.8    1  1.500   \n",
       "2           2          1  21.0  45.129412             1.0  9.2    1  2.125   \n",
       "3           3          1  21.0  45.129412             1.0  8.0    1  1.375   \n",
       "4           4          1  24.0  33.000000             1.0  9.3    1  1.571   \n",
       "\n",
       "   Education  Residence  Result  \n",
       "0        5.0        1.0       0  \n",
       "1        5.0        1.0       0  \n",
       "2        5.0        1.0       0  \n",
       "3        5.0        1.0       0  \n",
       "4        5.0        1.0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputing the dataset\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 96 entries, 0 to 95\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      96 non-null     int64  \n",
      " 1   Community       96 non-null     int64  \n",
      " 2   Age             96 non-null     float64\n",
      " 3   Weight          96 non-null     float64\n",
      " 4   Delivery phase  96 non-null     float64\n",
      " 5   HB              96 non-null     float64\n",
      " 6   IFA             96 non-null     int64  \n",
      " 7   BP              96 non-null     float64\n",
      " 8   Education       96 non-null     float64\n",
      " 9   Residence       96 non-null     float64\n",
      " 10  Result          96 non-null     int64  \n",
      "dtypes: float64(7), int64(4)\n",
      "memory usage: 8.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "Community         0\n",
       "Age               0\n",
       "Weight            0\n",
       "Delivery phase    0\n",
       "HB                0\n",
       "IFA               0\n",
       "BP                0\n",
       "Education         0\n",
       "Residence         0\n",
       "Result            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of null values column wise\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0:96\n",
      "Community:4\n",
      "Age:15\n",
      "Weight:29\n",
      "Delivery phase:2\n",
      "HB:22\n",
      "IFA:2\n",
      "BP:21\n",
      "Education:1\n",
      "Residence:2\n",
      "Result:2\n"
     ]
    }
   ],
   "source": [
    "#Inspect unique values in every feature\n",
    "for col in data.columns:\n",
    "    print(\"{}:{}\".format(col,data[col].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Community</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Delivery phase</th>\n",
       "      <th>HB</th>\n",
       "      <th>IFA</th>\n",
       "      <th>BP</th>\n",
       "      <th>Education</th>\n",
       "      <th>Residence</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.0</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>96.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>2.177083</td>\n",
       "      <td>23.752809</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.020833</td>\n",
       "      <td>9.061458</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>1.725189</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.135417</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>27.856777</td>\n",
       "      <td>1.213909</td>\n",
       "      <td>3.196317</td>\n",
       "      <td>7.725267</td>\n",
       "      <td>0.143576</td>\n",
       "      <td>0.705523</td>\n",
       "      <td>0.465946</td>\n",
       "      <td>1.330104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.343964</td>\n",
       "      <td>0.435286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>23.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>47.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>71.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>25.250000</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>95.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  Community        Age     Weight  Delivery phase         HB  \\\n",
       "count   96.000000  96.000000  96.000000  96.000000       96.000000  96.000000   \n",
       "mean    47.500000   2.177083  23.752809  45.129412        1.020833   9.061458   \n",
       "std     27.856777   1.213909   3.196317   7.725267        0.143576   0.705523   \n",
       "min      0.000000   1.000000  17.000000  30.000000        1.000000   5.900000   \n",
       "25%     23.750000   1.000000  21.000000  40.000000        1.000000   9.000000   \n",
       "50%     47.500000   2.500000  24.000000  45.000000        1.000000   9.000000   \n",
       "75%     71.250000   3.000000  25.250000  49.250000        1.000000   9.200000   \n",
       "max     95.000000   4.000000  38.000000  65.000000        2.000000  11.000000   \n",
       "\n",
       "             IFA         BP  Education  Residence     Result  \n",
       "count  96.000000  96.000000       96.0  96.000000  96.000000  \n",
       "mean    0.687500   1.725189        5.0   1.135417   0.750000  \n",
       "std     0.465946   1.330104        0.0   0.343964   0.435286  \n",
       "min     0.000000   1.200000        5.0   1.000000   0.000000  \n",
       "25%     0.000000   1.375000        5.0   1.000000   0.750000  \n",
       "50%     1.000000   1.571429        5.0   1.000000   1.000000  \n",
       "75%     1.000000   1.714286        5.0   1.000000   1.000000  \n",
       "max     1.000000  13.875000        5.0   2.000000   1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Basic statistics feautre wise\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    47\n",
       "3    32\n",
       "4    16\n",
       "2     1\n",
       "Name: Community, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Community'].value_counts()\n",
    "#NO null values in community column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    83\n",
       "2.0    13\n",
       "Name: Residence, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Residence'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIll Age ,Weight ,HB and BP  Null values with mean\n",
    "# Delivery phase with 1 since the coulmn takes binary values\n",
    "# Education with 5\n",
    "#Residence with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Community', 'Age', 'Weight', 'Delivery phase', 'HB',\n",
       "       'IFA', 'BP', 'Education', 'Residence', 'Result'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns\n",
    "#returns all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.061458333333333\n"
     ]
    }
   ],
   "source": [
    "#used to fill the \"HB\" feature by mean\n",
    "x=data['HB'].mean()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dict containing column wise values used to fill the null values\n",
    "values = { 'Age':data['Age'].mean(), 'Weight':data['Weight'].mean(), 'Delivery phase':1, 'HB':9.0, 'BP':data['BP'].mean(),\n",
    "       'Education':5, 'Residence':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna(value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Community</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Delivery phase</th>\n",
       "      <th>HB</th>\n",
       "      <th>IFA</th>\n",
       "      <th>BP</th>\n",
       "      <th>Education</th>\n",
       "      <th>Residence</th>\n",
       "      <th>Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.375</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.500</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.129412</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.125</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Community   Age     Weight  Delivery phase   HB  IFA     BP  \\\n",
       "0           0          1  21.0  42.000000             1.0  9.2    1  1.375   \n",
       "1           1          1  21.0  45.129412             1.0  8.8    1  1.500   \n",
       "2           2          1  21.0  45.129412             1.0  9.2    1  2.125   \n",
       "\n",
       "   Education  Residence  Result  \n",
       "0        5.0        1.0       0  \n",
       "1        5.0        1.0       0  \n",
       "2        5.0        1.0       0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        0\n",
       "Community         0\n",
       "Age               0\n",
       "Weight            0\n",
       "Delivery phase    0\n",
       "HB                0\n",
       "IFA               0\n",
       "BP                0\n",
       "Education         0\n",
       "Residence         0\n",
       "Result            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no nulls values the dataset is now cleaned\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "#activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    72\n",
       "0    24\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Result'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(columns='Education')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#75% of the result column is 1 so the model might over fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "class NN:\n",
    "    \n",
    "    ''' X and Y are dataframes'''\n",
    "    def __init__(self,numHlayers=2):\n",
    "        \n",
    "        #number of hidden layers the ANN has\n",
    "        self.numHlayers = numHlayers\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by talking x_train and y_train samples as input\n",
    "        '''\n",
    "        input1=X\n",
    "        result1=Y\n",
    "        \n",
    "        #If the ANN has two hidden layer we initialize it calling ANN_layer class\n",
    "        \n",
    "        if(self.numHlayers==2):\n",
    "            \n",
    "            #First layer is initialized with parameter 1 :Number of inputs\n",
    "            #                                parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "            self.Hlayer1 =ANN_layers(7,24)\n",
    "            self.Hlayer2 =ANN_layers(24,24)\n",
    "            self.Output_layer =ANN_layers(24,2)\n",
    "            \n",
    "            #Between each layer we have the activation function class with a \n",
    "            #wide range of activation functions \n",
    "            \n",
    "            self.act1=activation()\n",
    "            self.act2=activation()\n",
    "            \n",
    "            # using the softmax function on the final layer \n",
    "            self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "            #So for optimizer we can choose between Adam and SGD optimizer\n",
    "            #Creating optimizer\n",
    "            self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "            \n",
    "            #Loop to train in\n",
    "            for epoch in range(10001):\n",
    "                \n",
    "                #pass input X through first layer\n",
    "                self.Hlayer1.fpropogation(input1)\n",
    "                \n",
    "                #Output of first layer passes through the firstactivation function\n",
    "                self.act1.fRelu(self.Hlayer1.output)\n",
    "                \n",
    "                #Output of first activation is sent in forward pass to second layer\n",
    "                self.Hlayer2.fpropogation(self.act1.output)\n",
    "                \n",
    "                 #Output ofsecond layer passes through the next activation function\n",
    "                self.act2.fRelu(self.Hlayer2.output)\n",
    "                \n",
    "                # Perform a forward pass through the activation/loss function\n",
    "                # takes the output of second dense layer here and returns loss\n",
    "                self.Output_layer.fpropogation(self.act2.output)\n",
    "                loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "                #Argmax returns the predictions for the foward pass\n",
    "                prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "                #calculate values along first axis\n",
    "                if len(result1.shape)==2:\n",
    "                    result1=np.argmax(result1,axis=1)\n",
    "                #returns the accuracy of our predictions in this pass\n",
    "                accuracy= np.mean(prediction==result1)\n",
    "                \n",
    "                #priniting for every 100Th epoch \n",
    "                if not epoch %100:\n",
    "                    print(f'epoch:{epoch} ,'+\n",
    "                          f'accuracy:{accuracy:.3f} ,'+\n",
    "                          f'loss:{loss} ,'+\n",
    "                          f'lr:{self.optimize.curr_l_rate},')\n",
    "                    \n",
    "                #back propogation \n",
    "                #The values are propogated backwards and the dinputs are updated\n",
    "                self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "                self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "                self.act2.back(self.Output_layer.dinputs)\n",
    "                self.Hlayer2.Bpropogation(self.act2.dinputs)\n",
    "                self.act1.back(self.Hlayer2.dinputs)\n",
    "                self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "                #Update in weights and the biases\n",
    "                self.optimize.initial_update_param()\n",
    "                self.optimize.update_params(self.Hlayer1)\n",
    "                self.optimize.update_params(self.Hlayer2)\n",
    "                self.optimize.update_params(self.Output_layer)\n",
    "                self.optimize.post_update_params()\n",
    "        \n",
    "        \n",
    "        #If ANN has 1 hidden layer we initialize it calling ANN_layer()\n",
    "        \n",
    "        if(self.numHlayers==1):\n",
    "            \n",
    "            # layers are initialized with parameter 1 :Number of inputs\n",
    "            #                             parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "            \n",
    "            self.Hlayer1 =ANN_layers(7,128)\n",
    "            self.Output_layer =ANN_layers(128,2)\n",
    "            #Between each layer we have the activation function class with a \n",
    "            #wide range of activation functions \n",
    "            \n",
    "            self.act1=activation()\n",
    "            \n",
    "            # using the softmax function on the final layer\n",
    "            self.loss_activation=Activation_softmax_LCCE()\n",
    "            \n",
    "            #So for optimizer we can choose between Adam and SGD optimizer\n",
    "            #Creating optimizer\n",
    "            self.optimize=optimizer(l_rate=0.05,decay=5e-7)\n",
    "            \n",
    "            #Loop to train in\n",
    "            for epoch in range(11001):\n",
    "                \n",
    "                \n",
    "                #pass input X through first layer\n",
    "                #Output of first layer passes through the firstactivation function\n",
    "                # Perform a forward pass through the activation/loss function\n",
    "                # takes the output of second dense layer here and returns loss\n",
    "                self.Hlayer1.fpropogation(input1)\n",
    "                self.act1.fRelu(self.Hlayer1.output)\n",
    "                self.Output_layer.fpropogation(self.act1.output)\n",
    "                loss = self.loss_activation.forward(self.Output_layer.output,result1)\n",
    "                \n",
    "                #Argmax returns the predictions for the foward pass\n",
    "                prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "                \n",
    "                #calculate values along first axis\n",
    "                if len(result1.shape)==2:\n",
    "                    result1=np.argmax(result1,axis=1)\n",
    "               \n",
    "                #returns the accuracy of our predictions in this pass\n",
    "                accuracy= np.mean(prediction==result1)\n",
    "               \n",
    "                \n",
    "                #priniting for every 100Th epoch\n",
    "                if not epoch %100:\n",
    "                    print(f'epoch:{epoch} ,'+\n",
    "                          f'accuracy:{accuracy:.3f} ,'+\n",
    "                          f'loss:{loss} ,'+\n",
    "                          f'lr:{self.optimize.curr_l_rate},')\n",
    "                \n",
    "                \n",
    "                #back propogation \n",
    "                #The values are propogated backwards and the dinputs are updated\n",
    "                \n",
    "                self.loss_activation.backpass(self.loss_activation.output,result1)\n",
    "                self.Output_layer.Bpropogation(self.loss_activation.dinputs)\n",
    "                self.act1.back(self.Output_layer.dinputs)\n",
    "                self.Hlayer1.Bpropogation(self.act1.dinputs)\n",
    "                \n",
    "                \n",
    "                #Update in weights and the biases\n",
    "\n",
    "                self.optimize.initial_update_param()\n",
    "                self.optimize.update_params(self.Hlayer1)\n",
    "                self.optimize.update_params(self.Output_layer)\n",
    "                self.optimize.post_update_params()\n",
    "                \n",
    "               \n",
    "    \n",
    "    \n",
    "    def predict(self,X,Y):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \n",
    "        Only Forward pass to be performed in already trained model\n",
    "        \"\"\"\n",
    "        \n",
    "        #Only Forward pass to be performed in already trained model\n",
    "        x_test=X\n",
    "        y_test=Y\n",
    "        #If ANN has 2 hidden layer we call the  ANN_layer() objects\n",
    "        if(self.numHlayers==2):\n",
    "            #pass input to the first layer \n",
    "            self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "            #layer 1 activation func\n",
    "            self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "            #output of act1 to 2nd layer\n",
    "            self.Hlayer2.fpropogation(self.act1.output)\n",
    "            \n",
    "            #layer 2 activation func\n",
    "            self.act2.fRelu(self.Hlayer2.output)\n",
    "            self.Output_layer.fpropogation(self.act2.output)\n",
    "            \n",
    "            #loss/activation for output layer\n",
    "            loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "            #Argmax returns our models predictions\n",
    "            prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "            if len(y_test.shape)==2:\n",
    "                result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "            #Accuracy out model has attained\n",
    "            accuracy= np.mean(prediction==y_test)\n",
    "            print(f'accuracy:{accuracy:.3f} ,'+\n",
    "                  f'lr:{self.optimize.curr_l_rate},')\n",
    "            \n",
    "            #return the prediction\n",
    "            yhat=prediction\n",
    "            return yhat\n",
    "       \n",
    "    \n",
    "        #If ANN has 1 hidden layer we call the  ANN_layer() objects\n",
    "        if(self.numHlayers==1):\n",
    "            #pass input to the first layer \n",
    "            self.Hlayer1.fpropogation(x_test)\n",
    "            \n",
    "            #layer 1 activation func\n",
    "            self.act1.fRelu(self.Hlayer1.output)\n",
    "            \n",
    "            self.Output_layer.fpropogation(self.act1.output)\n",
    "            #loss/activation for output layer\n",
    "            loss = self.loss_activation.forward(self.Output_layer.output,y_test)\n",
    "            \n",
    "            #Argmax returns our models predictions\n",
    "            prediction = np.argmax(self.loss_activation.output,axis=1)\n",
    "            if len(y_test.shape)==2:\n",
    "                result1=np.argmax(y_test,axis=1)\n",
    "            \n",
    "            #Accuracy out model has attained\n",
    "            accuracy= np.mean(prediction==y_test)\n",
    "            print(f'accuracy:{accuracy:.3f} ,'+\n",
    "                  f'lr:{self.optimize.curr_l_rate},')\n",
    "            #return the prediction\n",
    "            yhat=prediction\n",
    "            return yhat\n",
    "        \n",
    "    \n",
    "    def CM(self,y_test,y_test_obs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Since our prediction are already given in list np.array we dont \n",
    "        need to convert confidence values\n",
    "        \"\"\"\n",
    "        #2x2 matrix \n",
    "        cm=np.array([[0,0],[0,0]])\n",
    "        fp=0 #false positive\n",
    "        fn=0 #false negative\n",
    "        tp=0 #true Positive\n",
    "        tn=0 #true negative\n",
    "        \n",
    "        #loop over number of samples\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "        \n",
    "        p= tp/(tp+fp) #precision\n",
    "        r=tp/(tp+fn)  #recall\n",
    "        f1=(2*p*r)/(p+r) #f1 score\n",
    "        \n",
    "        \n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer class so each layer can be intilialised as an object \n",
    "class ANN_layers:\n",
    "    \n",
    "    # layer properties\n",
    "    def __init__(self,num_inputs ,num_cells):\n",
    "        \n",
    "        #initializing weights and biases\n",
    "        self.weights = 0.1 * np.random.randn(num_inputs,num_cells) #in range[-1,1]\n",
    "        self.biases = np.zeros((1,num_cells))\n",
    "    \n",
    "    #forward pass\n",
    "    def fpropogation(self , inputs):\n",
    "        \n",
    "        #forward pass\n",
    "        self.inputs= inputs\n",
    "        \n",
    "        #Calculate output values from inputs,weights and biases\n",
    "        self.output= np.dot(inputs ,self.weights)\n",
    "    \n",
    "    #back pass\n",
    "    def Bpropogation(self,dval):\n",
    "        #gradient on parameters and values\n",
    "        self.dweights= np.dot(self.inputs.T,dval)\n",
    "        self.dbiases = np.sum(dval,axis=0,keepdims=True)\n",
    "        self.dinputs =np.dot(dval,self.weights.T)\n",
    "\n",
    "def confidenceTolist(x):\n",
    "    #to convert a given confidence matrix to list of predictions\n",
    "    rows = x.shape[0]\n",
    "    cols = x.shape[1]\n",
    "    predict=[None]*rows\n",
    "    for i in range(0,rows):\n",
    "        if(x[i,0]>x[i,1]):\n",
    "            predict[i]=0\n",
    "        else:\n",
    "            predict[i]=1\n",
    "    return predict\n",
    "        \n",
    "        \n",
    "    \n",
    "#Activation class with different activation func        \n",
    "class activation:\n",
    "    \n",
    "    #RELU activation\n",
    "    #makes copy of input values\n",
    "    def fRelu(self,x):\n",
    "        self.inputs=x\n",
    "        self.output = np.maximum(0,x)\n",
    "        #sigmoid\n",
    "        #self.inputs =x\n",
    "        #self.output = 1/(1+np.exp(-x))\n",
    "        #self.output = np.tanh(x) #tanh\n",
    "    \n",
    "    #sigmoid activation\n",
    "    def sigmoid(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = 1/(1+np.exp(-x))\n",
    "    \n",
    "    #tanh activation\n",
    "    def tanh(self,x):\n",
    "        self.inputs =x\n",
    "        self.output = np.tanh(x)\n",
    "        \n",
    "    #for back pass\n",
    "    def back(self,dval):\n",
    "        #copy of back pass inputs\n",
    "        self.dinputs = dval.copy()\n",
    "        \n",
    "        #Gradient to be set ) for -ve values\n",
    "        self.dinputs[self.inputs <=0] =0 \n",
    "        \n",
    "#Softmax activation\n",
    "class act_softmax :       \n",
    "    \n",
    "    #forward pass\n",
    "    def fsoftmax(self,inputs):\n",
    "        \n",
    "        #make copy of input values\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        #unnormalised probabities\n",
    "        exp1 = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        \n",
    "        #normalising\n",
    "        prob = exp1 /np.sum(exp1 ,axis=1, keepdims=True)\n",
    "        \n",
    "         #storing and calculating output\n",
    "        self.output = prob\n",
    "    \n",
    "    #Back pass\n",
    "    def bsoftmax(self,dval):\n",
    "        \n",
    "        #create empty array\n",
    "        self.dinputs = np.empty_like(dval)\n",
    "        \n",
    "        #enumerate output and gradients\n",
    "        for i ,(single_o,single_dval) in enumerate(zip(self.output,dval)):\n",
    "            \n",
    "            #reshape output array\n",
    "            single_o = single_o.reshape(-1,1)\n",
    "            \n",
    "            #Calculate the jacobian matrix of the output\n",
    "            jacobian = np.diagflat(single_o)-np.dot(single_o,single_o.T)\n",
    "            \n",
    "            self.dinputs[i] = np.dot(jacobian,single_dval)\n",
    "\n",
    "#Loss class        \n",
    "class loss:\n",
    "    \n",
    "    #for given model output and ground truth values calculate loss\n",
    "    def cal(self,output,y):\n",
    "        \n",
    "        sampleLosses = self.check(output,y)\n",
    "        \n",
    "        #average loss\n",
    "        data_loss= np.mean(sampleLosses)\n",
    "        \n",
    "        #return the loss\n",
    "        return data_loss\n",
    "\n",
    "#Cross Entropy Loss  inheriting the loss class    \n",
    "class CCE(loss):\n",
    "    #for forward pass\n",
    "    def check(self,y_pred,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x = len(y_pred)\n",
    "        \n",
    "        #clip data to prevent divison by 0\n",
    "        y_pred_clipped = np.clip(y_pred ,1e-7,1 - 1e-7)\n",
    "        \n",
    "        #probablities for target values\n",
    "        confidence = y_pred_clipped[range(x),y_true]\n",
    "        negLikely = -np.log(confidence)\n",
    "        \n",
    "        #Return losses\n",
    "        return negLikely\n",
    "    \n",
    "    #for back pass\n",
    "    def backward(self,dval,y_true):\n",
    "        \n",
    "        #number of samples\n",
    "        x=len(dval)\n",
    "        \n",
    "        # Number of labels in every sample\n",
    "        # use the first sample to count them\n",
    "        \n",
    "        label=len(dval[0])\n",
    "        \n",
    "        if len(y_true.shape)==1:\n",
    "            y_true=np.eye(label)[y_true] #\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs =-y_true/dval\n",
    "        # Normalize gradient\n",
    "        self.dinputs =self.dinputs/x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer: #SGD\n",
    "    \n",
    "    #initialize optimizer \n",
    "    #set default settings\n",
    "    def __init__(self , l_rate=1.,decay=0.,p=0.):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.p =p\n",
    "        \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass\n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "            \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #if momentum is used\n",
    "        if self.p:\n",
    "            \n",
    "            #if the layer doesnt have momentum atrribute create them filled with 0\n",
    "            \n",
    "            if not hasattr(layer,'wieght_momentums'):\n",
    "                layer.weight_p = np.zeros_like(layer.weights)\n",
    "                layer.bias_p = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            \n",
    "            \n",
    "            \n",
    "            w_updates= self.p*layer.weight_p - self_curr_l_rate*layer.dweights\n",
    "            layer.weight_p = w_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            b_updates= self.p*layer.bias_p - self_curr_l_rate*layer.dbiases\n",
    "            layer.bias_p = b_updates\n",
    "        \n",
    "        # SGD updates (as before momentum update)\n",
    "        else:\n",
    "            w_updates =-self.curr_l_rate * layer.dweights\n",
    "            b_updates =-self.curr_l_rate * layer.dbiases\n",
    "        \n",
    "        # Update weights and biases using either\n",
    "        #  momentum updates\n",
    "        \n",
    "        \n",
    "        layer.weights+=w_updates\n",
    "        layer.biases+=b_updates\n",
    "    \n",
    "    \n",
    "    #update iterations after every update    \n",
    "    def post_update_params(self):\n",
    "        self.iter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adam optimizer        \n",
    "class  Adam_op:\n",
    "    \n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self , l_rate=.001,decay=0.,ep=1e-7,b1=0.9,b2=0.999):\n",
    "        self.l_rate = l_rate\n",
    "        self.curr_l_rate=l_rate\n",
    "        self.decay = decay\n",
    "        self.iter =0\n",
    "        self.ep =ep\n",
    "        self.b1=b1\n",
    "        self.b2=b2\n",
    "    \n",
    "    #Call before any parameters are updated\n",
    "    #After every back pass\n",
    "    def initial_update_param(self):\n",
    "        if self.decay:\n",
    "            self.curr_l_rate = self.l_rate *(1./(1+self.decay*self.iter))\n",
    "            \n",
    "    #Update the parameters\n",
    "    def update_params(self,layer):\n",
    "        \n",
    "        #If layer does not contain cache arrays,\n",
    "        # create them filled with zeros\n",
    "        if not hasattr(layer,'w_cache'):\n",
    "            layer.w_p =np.zeros_like(layer.weights)\n",
    "            layer.w_cache=np.zeros_like(layer.weights)\n",
    "            layer.bias_p=np.zeros_like(layer.biases)\n",
    "            layer.bias_cache=np.zeros_like(layer.biases)\n",
    "            \n",
    "        #Update momentum with current gradients\n",
    "        layer.w_p = self.b1*layer.w_p + (1-self.b1)*layer.dweights\n",
    "        layer.bias_p=self.b1*layer.bias_p + (1-self.b1)*layer.dbiases\n",
    "        \n",
    "        # Get corrected momentum\n",
    "        # self.iteration is 0 at first pass\n",
    "        # and we need to start with 1 here\n",
    "        w_p_corrected = layer.w_p/(1-self.b1**(self.iter +1))\n",
    "        bias_p_corrected = layer.bias_p/(1-self.b1**(self.iter +1))\n",
    "        \n",
    "        # Update cache with squared current gradients\n",
    "        layer.w_cache = self.b2 * layer.w_cache + (1-self.b2)*layer.dweights**2\n",
    "        layer.bias_cache = self.b2*layer.bias_cache+(1-self.b2)*layer.dbiases**2\n",
    "        \n",
    "        # Get corrected cache\n",
    "        w_cache_corrected = layer.w_cache/(1-self.b2)**(self.iter+1)\n",
    "        bias_cache_corrected = layer.bias_cache/(1-self.b2)**(self.iter+1)\n",
    "        \n",
    "        \n",
    "        # SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        \n",
    "        layer.weights+=-self.curr_l_rate*w_p_corrected/(np.sqrt(w_cache_corrected)+self.ep)\n",
    "        layer.biases+=-self.curr_l_rate*bias_p_corrected/(np.sqrt(bias_cache_corrected)+self.ep)\n",
    "    \n",
    "    #update iterations after every update    \n",
    "    def post_update_params(self):\n",
    "        self.iter+=1\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_softmax_LCCE():\n",
    "    \n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation=act_softmax()\n",
    "        self.loss=CCE()\n",
    "    \n",
    "    # Forward pass\n",
    "    def forward(self,inputs,y_true):\n",
    "        \n",
    "        # Output layer's activation function\n",
    "        self.activation.fsoftmax(inputs)\n",
    "        \n",
    "        #store the output\n",
    "        self.output=self.activation.output\n",
    "        \n",
    "        #Calculate and return loss value\n",
    "        return self.loss.cal(self.output,y_true)\n",
    "        \n",
    "    # Backward pass\n",
    "    def backpass(self,dval,y_true):\n",
    "        \n",
    "        # Number of samples\n",
    "        x=len(dval)\n",
    "        \n",
    "        \n",
    "        if len(y_true.shape)==2:\n",
    "            y_true = np.argmax(y_true,axis=1)\n",
    "        \n",
    "        \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs =dval.copy()\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(x),y_true]-=1\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs= self.dinputs/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input2=data.loc[:,['Age', 'Weight', 'Delivery phase', 'HB', 'IFA', 'BP', 'Residence']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=data['Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing test train split\n",
    "input1,x_test, result1, y_test = train_test_split(input2, result, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating first model\n",
    "#If the ANN has two hidden layer we initialize it calling ANN_layer class\n",
    "#First layer is initialized with parameter 1 :Number of inputs\n",
    "            #                   parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "Hlayer1=ANN_layers(7,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hlayer2=ANN_layers(16,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_layer=ANN_layers(16,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Between each layer we have the activation function class with a \n",
    "#wide range of activation functions \n",
    "act1=activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "act2=activation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the softmax function on the final layer \n",
    "loss_activation=Activation_softmax_LCCE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So for optimizer we can choose between Adam and SGD optimizer\n",
    "#Creating optimizer\n",
    "optimize = optimizer(l_rate=0.025,decay=5e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ,accuracy:0.339 ,loss:0.7339040423240516 ,lr:0.025,\n",
      "epoch:100 ,accuracy:0.790 ,loss:0.4895713225451092 ,lr:0.02499876256125322,\n",
      "epoch:200 ,accuracy:0.790 ,loss:0.4811253595825717 ,lr:0.02499751274748163,\n",
      "epoch:300 ,accuracy:0.806 ,loss:0.4752756782995178 ,lr:0.024996263058672728,\n",
      "epoch:400 ,accuracy:0.806 ,loss:0.4693595196997094 ,lr:0.02499501349480779,\n",
      "epoch:500 ,accuracy:0.806 ,loss:0.46764296970033425 ,lr:0.024993764055868062,\n",
      "epoch:600 ,accuracy:0.806 ,loss:0.45900947084058913 ,lr:0.024992514741834823,\n",
      "epoch:700 ,accuracy:0.806 ,loss:0.45483294041106703 ,lr:0.024991265552689337,\n",
      "epoch:800 ,accuracy:0.806 ,loss:0.4519772376908679 ,lr:0.024990016488412876,\n",
      "epoch:900 ,accuracy:0.806 ,loss:0.4474648582545686 ,lr:0.024988767548986733,\n",
      "epoch:1000 ,accuracy:0.823 ,loss:0.44676850931188955 ,lr:0.024987518734392172,\n",
      "epoch:1100 ,accuracy:0.806 ,loss:0.4536969050608614 ,lr:0.024986270044610487,\n",
      "epoch:1200 ,accuracy:0.806 ,loss:0.4442178014321334 ,lr:0.024985021479622966,\n",
      "epoch:1300 ,accuracy:0.823 ,loss:0.4172322715233065 ,lr:0.024983773039410906,\n",
      "epoch:1400 ,accuracy:0.806 ,loss:0.440467972706289 ,lr:0.024982524723955592,\n",
      "epoch:1500 ,accuracy:0.823 ,loss:0.3909725196365269 ,lr:0.02498127653323834,\n",
      "epoch:1600 ,accuracy:0.790 ,loss:0.44438650928048934 ,lr:0.024980028467240442,\n",
      "epoch:1700 ,accuracy:0.839 ,loss:0.36275565873216603 ,lr:0.02497878052594321,\n",
      "epoch:1800 ,accuracy:0.839 ,loss:0.36618966680317505 ,lr:0.024977532709327958,\n",
      "epoch:1900 ,accuracy:0.823 ,loss:0.39654257737990445 ,lr:0.024976285017376004,\n",
      "epoch:2000 ,accuracy:0.855 ,loss:0.35860584854330857 ,lr:0.024975037450068654,\n",
      "epoch:2100 ,accuracy:0.839 ,loss:0.43595658647734287 ,lr:0.02497379000738725,\n",
      "epoch:2200 ,accuracy:0.855 ,loss:0.3745497535957289 ,lr:0.0249725426893131,\n",
      "epoch:2300 ,accuracy:0.855 ,loss:0.3258258847468222 ,lr:0.02497129549582755,\n",
      "epoch:2400 ,accuracy:0.839 ,loss:0.32501212236279126 ,lr:0.02497004842691192,\n",
      "epoch:2500 ,accuracy:0.855 ,loss:0.33194290990591063 ,lr:0.02496880148254756,\n",
      "epoch:2600 ,accuracy:0.839 ,loss:0.34135439752422225 ,lr:0.024967554662715802,\n",
      "epoch:2700 ,accuracy:0.823 ,loss:0.39642609524243577 ,lr:0.024966307967398002,\n",
      "epoch:2800 ,accuracy:0.855 ,loss:0.30946827983948416 ,lr:0.02496506139657549,\n",
      "epoch:2900 ,accuracy:0.871 ,loss:0.32258559559846367 ,lr:0.024963814950229642,\n",
      "epoch:3000 ,accuracy:0.887 ,loss:0.3069825179243331 ,lr:0.024962568628341803,\n",
      "epoch:3100 ,accuracy:0.887 ,loss:0.29724604101448615 ,lr:0.02496132243089333,\n",
      "epoch:3200 ,accuracy:0.823 ,loss:0.3654927401251556 ,lr:0.024960076357865596,\n",
      "epoch:3300 ,accuracy:0.855 ,loss:0.36268335856401807 ,lr:0.02495883040923996,\n",
      "epoch:3400 ,accuracy:0.855 ,loss:0.34667942064059437 ,lr:0.024957584584997798,\n",
      "epoch:3500 ,accuracy:0.839 ,loss:0.307232956043935 ,lr:0.024956338885120482,\n",
      "epoch:3600 ,accuracy:0.855 ,loss:0.34097909465660703 ,lr:0.024955093309589397,\n",
      "epoch:3700 ,accuracy:0.855 ,loss:0.30443114594895226 ,lr:0.024953847858385916,\n",
      "epoch:3800 ,accuracy:0.855 ,loss:0.2966497534415992 ,lr:0.024952602531491435,\n",
      "epoch:3900 ,accuracy:0.887 ,loss:0.3173669072663498 ,lr:0.024951357328887334,\n",
      "epoch:4000 ,accuracy:0.855 ,loss:0.3065249050184871 ,lr:0.024950112250555018,\n",
      "epoch:4100 ,accuracy:0.855 ,loss:0.33397855507386276 ,lr:0.02494886729647587,\n",
      "epoch:4200 ,accuracy:0.887 ,loss:0.33425190755176515 ,lr:0.02494762246663131,\n",
      "epoch:4300 ,accuracy:0.839 ,loss:0.3243773319409002 ,lr:0.024946377761002725,\n",
      "epoch:4400 ,accuracy:0.790 ,loss:0.4415302189710763 ,lr:0.024945133179571536,\n",
      "epoch:4500 ,accuracy:0.823 ,loss:0.34689124018162765 ,lr:0.024943888722319146,\n",
      "epoch:4600 ,accuracy:0.887 ,loss:0.29149471380822206 ,lr:0.024942644389226977,\n",
      "epoch:4700 ,accuracy:0.839 ,loss:0.3312124237275374 ,lr:0.024941400180276442,\n",
      "epoch:4800 ,accuracy:0.839 ,loss:0.34917299380789196 ,lr:0.02494015609544897,\n",
      "epoch:4900 ,accuracy:0.887 ,loss:0.2866273192945307 ,lr:0.024938912134725988,\n",
      "epoch:5000 ,accuracy:0.903 ,loss:0.3305834366583332 ,lr:0.024937668298088925,\n",
      "epoch:5100 ,accuracy:0.839 ,loss:0.3271793618998597 ,lr:0.02493642458551922,\n",
      "epoch:5200 ,accuracy:0.806 ,loss:0.4413881183576383 ,lr:0.024935180996998305,\n",
      "epoch:5300 ,accuracy:0.855 ,loss:0.3141700722537036 ,lr:0.024933937532507625,\n",
      "epoch:5400 ,accuracy:0.903 ,loss:0.27933089662094945 ,lr:0.02493269419202862,\n",
      "epoch:5500 ,accuracy:0.871 ,loss:0.28121265146469376 ,lr:0.024931450975542748,\n",
      "epoch:5600 ,accuracy:0.855 ,loss:0.29905703278999146 ,lr:0.024930207883031453,\n",
      "epoch:5700 ,accuracy:0.871 ,loss:0.2831812464836106 ,lr:0.0249289649144762,\n",
      "epoch:5800 ,accuracy:0.855 ,loss:0.31641933686142404 ,lr:0.024927722069858445,\n",
      "epoch:5900 ,accuracy:0.855 ,loss:0.3169656373785836 ,lr:0.024926479349159657,\n",
      "epoch:6000 ,accuracy:0.839 ,loss:0.3144936272321607 ,lr:0.02492523675236129,\n",
      "epoch:6100 ,accuracy:0.839 ,loss:0.3043636285446382 ,lr:0.024923994279444837,\n",
      "epoch:6200 ,accuracy:0.871 ,loss:0.2903373005059773 ,lr:0.024922751930391753,\n",
      "epoch:6300 ,accuracy:0.839 ,loss:0.3890877885958045 ,lr:0.024921509705183528,\n",
      "epoch:6400 ,accuracy:0.871 ,loss:0.3145421538730928 ,lr:0.024920267603801637,\n",
      "epoch:6500 ,accuracy:0.871 ,loss:0.29024918451193016 ,lr:0.024919025626227578,\n",
      "epoch:6600 ,accuracy:0.887 ,loss:0.26135043306490097 ,lr:0.024917783772442827,\n",
      "epoch:6700 ,accuracy:0.823 ,loss:0.34124575202671786 ,lr:0.02491654204242889,\n",
      "epoch:6800 ,accuracy:0.871 ,loss:0.2601518955061567 ,lr:0.02491530043616725,\n",
      "epoch:6900 ,accuracy:0.855 ,loss:0.2797141286903277 ,lr:0.02491405895363942,\n",
      "epoch:7000 ,accuracy:0.855 ,loss:0.2869917377054491 ,lr:0.024912817594826903,\n",
      "epoch:7100 ,accuracy:0.855 ,loss:0.29796855118236476 ,lr:0.024911576359711203,\n",
      "epoch:7200 ,accuracy:0.871 ,loss:0.2803526520059576 ,lr:0.024910335248273838,\n",
      "epoch:7300 ,accuracy:0.790 ,loss:0.4619646505044499 ,lr:0.02490909426049632,\n",
      "epoch:7400 ,accuracy:0.855 ,loss:0.3706639003651166 ,lr:0.024907853396360168,\n",
      "epoch:7500 ,accuracy:0.823 ,loss:0.36521082929803417 ,lr:0.0249066126558469,\n",
      "epoch:7600 ,accuracy:0.823 ,loss:0.3866737792437542 ,lr:0.024905372038938056,\n",
      "epoch:7700 ,accuracy:0.806 ,loss:0.3883603036441767 ,lr:0.024904131545615153,\n",
      "epoch:7800 ,accuracy:0.855 ,loss:0.332241653570467 ,lr:0.02490289117585974,\n",
      "epoch:7900 ,accuracy:0.887 ,loss:0.3114818604268073 ,lr:0.024901650929653334,\n",
      "epoch:8000 ,accuracy:0.887 ,loss:0.3183229063272469 ,lr:0.024900410806977497,\n",
      "epoch:8100 ,accuracy:0.806 ,loss:0.5217320908326396 ,lr:0.02489917080781376,\n",
      "epoch:8200 ,accuracy:0.903 ,loss:0.30309539796994817 ,lr:0.02489793093214368,\n",
      "epoch:8300 ,accuracy:0.855 ,loss:0.34558390974516634 ,lr:0.024896691179948804,\n",
      "epoch:8400 ,accuracy:0.887 ,loss:0.317018708157856 ,lr:0.024895451551210694,\n",
      "epoch:8500 ,accuracy:0.887 ,loss:0.31498629001075173 ,lr:0.024894212045910902,\n",
      "epoch:8600 ,accuracy:0.855 ,loss:0.33600322255916804 ,lr:0.024892972664031003,\n",
      "epoch:8700 ,accuracy:0.887 ,loss:0.3128115569706693 ,lr:0.02489173340555255,\n",
      "epoch:8800 ,accuracy:0.887 ,loss:0.29408694896785476 ,lr:0.024890494270457128,\n",
      "epoch:8900 ,accuracy:0.887 ,loss:0.32034004650028824 ,lr:0.0248892552587263,\n",
      "epoch:9000 ,accuracy:0.887 ,loss:0.2987391756731333 ,lr:0.024888016370341645,\n",
      "epoch:9100 ,accuracy:0.871 ,loss:0.32840108568674115 ,lr:0.02488677760528476,\n",
      "epoch:9200 ,accuracy:0.871 ,loss:0.3262986861336633 ,lr:0.024885538963537207,\n",
      "epoch:9300 ,accuracy:0.871 ,loss:0.31291589537226205 ,lr:0.0248843004450806,\n",
      "epoch:9400 ,accuracy:0.887 ,loss:0.3270678154373179 ,lr:0.02488306204989651,\n",
      "epoch:9500 ,accuracy:0.903 ,loss:0.29429498483625655 ,lr:0.02488182377796655,\n",
      "epoch:9600 ,accuracy:0.839 ,loss:0.3557517486761752 ,lr:0.024880585629272308,\n",
      "epoch:9700 ,accuracy:0.887 ,loss:0.2996175733386674 ,lr:0.0248793476037954,\n",
      "epoch:9800 ,accuracy:0.887 ,loss:0.2915944642373955 ,lr:0.024878109701517416,\n",
      "epoch:9900 ,accuracy:0.887 ,loss:0.2862538684535644 ,lr:0.024876871922419982,\n",
      "epoch:10000 ,accuracy:0.887 ,loss:0.28991704708335125 ,lr:0.02487563426648471,\n"
     ]
    }
   ],
   "source": [
    "#Loop to train in\n",
    "for epoch in range(10001):\n",
    "    \n",
    "    #pass input X through first layer\n",
    "    Hlayer1.fpropogation(input1)\n",
    "    \n",
    "    #Output of first layer passes through the firstactivation function\n",
    "    act1.fRelu(Hlayer1.output)\n",
    "    \n",
    "    #Output of first activation is sent in forward pass to second layer\n",
    "    Hlayer2.fpropogation(act1.output)\n",
    "    \n",
    "    #Output ofsecond layer passes through the next activation function\n",
    "    act2.fRelu(Hlayer2.output)\n",
    "    \n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    Output_layer.fpropogation(act2.output)\n",
    "    loss = loss_activation.forward(Output_layer.output,result1)\n",
    "    \n",
    "    #Argmax returns the predictions for the foward pass\n",
    "    prediction = np.argmax(loss_activation.output,axis=1)\n",
    "    \n",
    "    #calculate values along first axis\n",
    "    if len(result1.shape)==2:\n",
    "        result1=np.argmax(result1,axis=1)\n",
    "    \n",
    "    \n",
    "    accuracy= np.mean(prediction==result1)\n",
    "    #returns the accuracy of our predictions in this pass\n",
    "    if not epoch %100:\n",
    "        print(f'epoch:{epoch} ,'+\n",
    "              f'accuracy:{accuracy:.3f} ,'+\n",
    "              f'loss:{loss} ,'+\n",
    "              f'lr:{optimize.curr_l_rate},')\n",
    "    \n",
    "    #back propogation \n",
    "    #The values are propogated backwards and the dinputs are updated\n",
    "    loss_activation.backpass(loss_activation.output,result1)\n",
    "    Output_layer.Bpropogation(loss_activation.dinputs)\n",
    "    act2.back(Output_layer.dinputs)\n",
    "    Hlayer2.Bpropogation(act2.dinputs)\n",
    "    act1.back(Hlayer2.dinputs)\n",
    "    Hlayer1.Bpropogation(act1.dinputs)\n",
    "              \n",
    "    #Update in weights and the biases\n",
    "    optimize.initial_update_param()\n",
    "    optimize.update_params(Hlayer1)\n",
    "    optimize.update_params(Hlayer2)\n",
    "    optimize.update_params(Output_layer)\n",
    "    optimize.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:10000 ,accuracy:0.676 ,lr:0.02487562189054727,\n"
     ]
    }
   ],
   "source": [
    "#Only Forward pass to be performed in already trained model\n",
    "\n",
    "#pass input to the first layer\n",
    "Hlayer1.fpropogation(x_test)\n",
    "\n",
    "#layer 1 activation func\n",
    "act1.fRelu(Hlayer1.output)\n",
    "\n",
    "#output of act1 to 2nd layer\n",
    "Hlayer2.fpropogation(act1.output)\n",
    "\n",
    "#layer 2 activation func\n",
    "act2.fRelu(Hlayer2.output)\n",
    "\n",
    "\n",
    "Output_layer.fpropogation(act2.output)\n",
    "\n",
    "#loss/activation for output layer\n",
    "loss = loss_activation.forward(Output_layer.output,y_test)\n",
    "\n",
    "#Argmax returns our models predictions\n",
    "prediction = np.argmax(loss_activation.output,axis=1)\n",
    "if len(y_test.shape)==2:\n",
    "    result1=np.argmax(y_test,axis=1)\n",
    "\n",
    "#Accuracy out model has attained    \n",
    "accuracy= np.mean(prediction==y_test)\n",
    "print(f'epoch:{epoch} ,'+f'accuracy:{accuracy:.3f} ,'+\n",
    "      f'lr:{optimize.curr_l_rate},')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[ 5  6]\n",
      " [ 5 18]]\n",
      "\n",
      "\n",
      "Precision : 0.75\n",
      "Recall : 0.782608695652174\n",
      "F1 SCORE : 0.7659574468085107\n"
     ]
    }
   ],
   "source": [
    "model1=NN() #to call confusion matrix func\n",
    "model1.CM(y_test.tolist(),prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since dataset is biased towards 1 we will try SMOTE approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    72\n",
       "0    48\n",
       "Name: Result, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=data[(data['Result']==0)]\n",
    "data2.head(3)\n",
    "new_data=pd.concat([data,data2])\n",
    "#new_data=pd.concat([new_data,data2])\n",
    "new_data.head(3)\n",
    "new_data=new_data.sample(frac=1)\n",
    "new_data.head(3)\n",
    "new_data['Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0 ,accuracy:0.410 ,loss:2.023 ,lr:0.02487562189054727,\n",
      "epoch:100 ,accuracy:0.679 ,loss:0.593 ,lr:0.024874384358987117,\n",
      "epoch:200 ,accuracy:0.718 ,loss:0.593 ,lr:0.02487314695055218,\n",
      "epoch:300 ,accuracy:0.782 ,loss:0.571 ,lr:0.024871909665224096,\n",
      "epoch:400 ,accuracy:0.744 ,loss:0.552 ,lr:0.02487067250298448,\n",
      "epoch:500 ,accuracy:0.756 ,loss:0.561 ,lr:0.02486943546381497,\n",
      "epoch:600 ,accuracy:0.756 ,loss:0.538 ,lr:0.024868198547697205,\n",
      "epoch:700 ,accuracy:0.744 ,loss:0.541 ,lr:0.024866961754612824,\n",
      "epoch:800 ,accuracy:0.731 ,loss:0.561 ,lr:0.024865725084543464,\n",
      "epoch:900 ,accuracy:0.744 ,loss:0.521 ,lr:0.024864488537470787,\n",
      "epoch:1000 ,accuracy:0.756 ,loss:0.541 ,lr:0.02486325211337643,\n",
      "epoch:1100 ,accuracy:0.756 ,loss:0.533 ,lr:0.02486201581224206,\n",
      "epoch:1200 ,accuracy:0.795 ,loss:0.507 ,lr:0.024860779634049326,\n",
      "epoch:1300 ,accuracy:0.769 ,loss:0.517 ,lr:0.024859543578779897,\n",
      "epoch:1400 ,accuracy:0.756 ,loss:0.513 ,lr:0.02485830764641543,\n",
      "epoch:1500 ,accuracy:0.808 ,loss:0.497 ,lr:0.02485707183693761,\n",
      "epoch:1600 ,accuracy:0.769 ,loss:0.508 ,lr:0.024855836150328096,\n",
      "epoch:1700 ,accuracy:0.667 ,loss:0.537 ,lr:0.024854600586568576,\n",
      "epoch:1800 ,accuracy:0.756 ,loss:0.495 ,lr:0.02485336514564072,\n",
      "epoch:1900 ,accuracy:0.782 ,loss:0.488 ,lr:0.024852129827526222,\n",
      "epoch:2000 ,accuracy:0.795 ,loss:0.469 ,lr:0.02485089463220676,\n",
      "epoch:2100 ,accuracy:0.782 ,loss:0.489 ,lr:0.024849659559664034,\n",
      "epoch:2200 ,accuracy:0.756 ,loss:0.510 ,lr:0.024848424609879738,\n",
      "epoch:2300 ,accuracy:0.679 ,loss:0.540 ,lr:0.02484718978283556,\n",
      "epoch:2400 ,accuracy:0.795 ,loss:0.469 ,lr:0.02484595507851322,\n",
      "epoch:2500 ,accuracy:0.782 ,loss:0.477 ,lr:0.024844720496894408,\n",
      "epoch:2600 ,accuracy:0.769 ,loss:0.486 ,lr:0.024843486037960848,\n",
      "epoch:2700 ,accuracy:0.782 ,loss:0.487 ,lr:0.02484225170169424,\n",
      "epoch:2800 ,accuracy:0.769 ,loss:0.491 ,lr:0.024841017488076312,\n",
      "epoch:2900 ,accuracy:0.756 ,loss:0.508 ,lr:0.024839783397088776,\n",
      "epoch:3000 ,accuracy:0.782 ,loss:0.480 ,lr:0.024838549428713365,\n",
      "epoch:3100 ,accuracy:0.769 ,loss:0.489 ,lr:0.0248373155829318,\n",
      "epoch:3200 ,accuracy:0.795 ,loss:0.462 ,lr:0.024836081859725814,\n",
      "epoch:3300 ,accuracy:0.795 ,loss:0.453 ,lr:0.024834848259077138,\n",
      "epoch:3400 ,accuracy:0.808 ,loss:0.444 ,lr:0.02483361478096752,\n",
      "epoch:3500 ,accuracy:0.769 ,loss:0.453 ,lr:0.024832381425378695,\n",
      "epoch:3600 ,accuracy:0.782 ,loss:0.470 ,lr:0.024831148192292415,\n",
      "epoch:3700 ,accuracy:0.756 ,loss:0.461 ,lr:0.024829915081690423,\n",
      "epoch:3800 ,accuracy:0.795 ,loss:0.444 ,lr:0.024828682093554478,\n",
      "epoch:3900 ,accuracy:0.795 ,loss:0.440 ,lr:0.02482744922786633,\n",
      "epoch:4000 ,accuracy:0.782 ,loss:0.456 ,lr:0.02482621648460775,\n",
      "epoch:4100 ,accuracy:0.782 ,loss:0.447 ,lr:0.02482498386376049,\n",
      "epoch:4200 ,accuracy:0.795 ,loss:0.455 ,lr:0.024823751365306325,\n",
      "epoch:4300 ,accuracy:0.795 ,loss:0.438 ,lr:0.02482251898922703,\n",
      "epoch:4400 ,accuracy:0.795 ,loss:0.445 ,lr:0.024821286735504367,\n",
      "epoch:4500 ,accuracy:0.769 ,loss:0.443 ,lr:0.02482005460412013,\n",
      "epoch:4600 ,accuracy:0.795 ,loss:0.446 ,lr:0.024818822595056092,\n",
      "epoch:4700 ,accuracy:0.846 ,loss:0.425 ,lr:0.024817590708294042,\n",
      "epoch:4800 ,accuracy:0.782 ,loss:0.437 ,lr:0.024816358943815762,\n",
      "epoch:4900 ,accuracy:0.833 ,loss:0.424 ,lr:0.024815127301603058,\n",
      "epoch:5000 ,accuracy:0.782 ,loss:0.449 ,lr:0.024813895781637715,\n",
      "epoch:5100 ,accuracy:0.769 ,loss:0.457 ,lr:0.024812664383901548,\n",
      "epoch:5200 ,accuracy:0.769 ,loss:0.454 ,lr:0.02481143310837634,\n",
      "epoch:5300 ,accuracy:0.795 ,loss:0.438 ,lr:0.024810201955043916,\n",
      "epoch:5400 ,accuracy:0.782 ,loss:0.438 ,lr:0.02480897092388608,\n",
      "epoch:5500 ,accuracy:0.782 ,loss:0.449 ,lr:0.024807740014884647,\n",
      "epoch:5600 ,accuracy:0.769 ,loss:0.460 ,lr:0.024806509228021432,\n",
      "epoch:5700 ,accuracy:0.769 ,loss:0.452 ,lr:0.02480527856327827,\n",
      "epoch:5800 ,accuracy:0.782 ,loss:0.442 ,lr:0.02480404802063697,\n",
      "epoch:5900 ,accuracy:0.846 ,loss:0.417 ,lr:0.024802817600079373,\n",
      "epoch:6000 ,accuracy:0.769 ,loss:0.449 ,lr:0.024801587301587304,\n",
      "epoch:6100 ,accuracy:0.782 ,loss:0.445 ,lr:0.024800357125142607,\n",
      "epoch:6200 ,accuracy:0.769 ,loss:0.456 ,lr:0.024799127070727112,\n",
      "epoch:6300 ,accuracy:0.795 ,loss:0.445 ,lr:0.02479789713832267,\n",
      "epoch:6400 ,accuracy:0.795 ,loss:0.441 ,lr:0.02479666732791113,\n",
      "epoch:6500 ,accuracy:0.821 ,loss:0.415 ,lr:0.024795437639474338,\n",
      "epoch:6600 ,accuracy:0.808 ,loss:0.425 ,lr:0.02479420807299415,\n",
      "epoch:6700 ,accuracy:0.782 ,loss:0.452 ,lr:0.024792978628452422,\n",
      "epoch:6800 ,accuracy:0.795 ,loss:0.425 ,lr:0.024791749305831022,\n",
      "epoch:6900 ,accuracy:0.769 ,loss:0.461 ,lr:0.024790520105111805,\n",
      "epoch:7000 ,accuracy:0.795 ,loss:0.435 ,lr:0.024789291026276652,\n",
      "epoch:7100 ,accuracy:0.769 ,loss:0.462 ,lr:0.02478806206930742,\n",
      "epoch:7200 ,accuracy:0.782 ,loss:0.448 ,lr:0.024786833234186,\n",
      "epoch:7300 ,accuracy:0.782 ,loss:0.437 ,lr:0.024785604520894264,\n",
      "epoch:7400 ,accuracy:0.782 ,loss:0.431 ,lr:0.0247843759294141,\n",
      "epoch:7500 ,accuracy:0.795 ,loss:0.438 ,lr:0.024783147459727387,\n",
      "epoch:7600 ,accuracy:0.795 ,loss:0.440 ,lr:0.02478191911181602,\n",
      "epoch:7700 ,accuracy:0.795 ,loss:0.436 ,lr:0.024780690885661892,\n",
      "epoch:7800 ,accuracy:0.821 ,loss:0.430 ,lr:0.024779462781246906,\n",
      "epoch:7900 ,accuracy:0.782 ,loss:0.459 ,lr:0.024778234798552953,\n",
      "epoch:8000 ,accuracy:0.808 ,loss:0.438 ,lr:0.02477700693756195,\n",
      "epoch:8100 ,accuracy:0.782 ,loss:0.440 ,lr:0.024775779198255785,\n",
      "epoch:8200 ,accuracy:0.769 ,loss:0.454 ,lr:0.02477455158061639,\n",
      "epoch:8300 ,accuracy:0.859 ,loss:0.403 ,lr:0.024773324084625677,\n",
      "epoch:8400 ,accuracy:0.705 ,loss:0.485 ,lr:0.024772096710265555,\n",
      "epoch:8500 ,accuracy:0.782 ,loss:0.434 ,lr:0.024770869457517962,\n",
      "epoch:8600 ,accuracy:0.833 ,loss:0.419 ,lr:0.024769642326364805,\n",
      "epoch:8700 ,accuracy:0.782 ,loss:0.434 ,lr:0.024768415316788035,\n",
      "epoch:8800 ,accuracy:0.833 ,loss:0.417 ,lr:0.024767188428769567,\n",
      "epoch:8900 ,accuracy:0.846 ,loss:0.406 ,lr:0.02476596166229135,\n",
      "epoch:9000 ,accuracy:0.782 ,loss:0.459 ,lr:0.024764735017335313,\n",
      "epoch:9100 ,accuracy:0.808 ,loss:0.421 ,lr:0.024763508493883417,\n",
      "epoch:9200 ,accuracy:0.808 ,loss:0.434 ,lr:0.02476228209191759,\n",
      "epoch:9300 ,accuracy:0.769 ,loss:0.468 ,lr:0.0247610558114198,\n",
      "epoch:9400 ,accuracy:0.756 ,loss:0.442 ,lr:0.02475982965237199,\n",
      "epoch:9500 ,accuracy:0.821 ,loss:0.420 ,lr:0.024758603614756132,\n",
      "epoch:9600 ,accuracy:0.833 ,loss:0.409 ,lr:0.02475737769855417,\n",
      "epoch:9700 ,accuracy:0.782 ,loss:0.442 ,lr:0.024756151903748084,\n",
      "epoch:9800 ,accuracy:0.795 ,loss:0.464 ,lr:0.024754926230319833,\n",
      "epoch:9900 ,accuracy:0.795 ,loss:0.431 ,lr:0.024753700678251403,\n",
      "epoch:10000 ,accuracy:0.859 ,loss:0.406 ,lr:0.024752475247524754,\n",
      "epoch:10100 ,accuracy:0.782 ,loss:0.435 ,lr:0.024751249938121878,\n",
      "epoch:10200 ,accuracy:0.795 ,loss:0.425 ,lr:0.02475002475002475,\n",
      "epoch:10300 ,accuracy:0.808 ,loss:0.429 ,lr:0.02474879968321536,\n",
      "epoch:10400 ,accuracy:0.846 ,loss:0.423 ,lr:0.02474757473767571,\n",
      "epoch:10500 ,accuracy:0.782 ,loss:0.435 ,lr:0.024746349913387772,\n",
      "epoch:10600 ,accuracy:0.782 ,loss:0.441 ,lr:0.024745125210333566,\n",
      "epoch:10700 ,accuracy:0.731 ,loss:0.470 ,lr:0.024743900628495075,\n",
      "epoch:10800 ,accuracy:0.821 ,loss:0.434 ,lr:0.02474267616785432,\n",
      "epoch:10900 ,accuracy:0.846 ,loss:0.383 ,lr:0.02474145182839329,\n",
      "epoch:11000 ,accuracy:0.769 ,loss:0.442 ,lr:0.024740227610094017,\n"
     ]
    }
   ],
   "source": [
    "new_input=new_data.loc[:,['Age', 'Weight', 'Delivery phase', 'HB', 'IFA', 'BP', 'Residence']]\n",
    "new_result=new_data['Result']\n",
    "#If ANN has 1 hidden layer we initialize it calling ANN_layer()\n",
    "\n",
    "# layers are initialized with parameter 1 :Number of inputs\n",
    "            #                parameter 2 :Number of neurons\n",
    "            #similarly hidden layer and output layer is initialised\n",
    "\n",
    "input11,x_test11, result11, y_test11 = train_test_split(new_input, new_result, test_size=0.35, random_state=42)\n",
    "Hlayer11=ANN_layers(7,128)\n",
    "#Hlayer12=ANN_layers(16,8)\n",
    "Output_layer1=ANN_layers(128,2)\n",
    "\n",
    "#Between each layer we have the activation function class with a \n",
    "#wide range of activation functions \n",
    "act11=activation()\n",
    "#act12=activation()\n",
    "\n",
    "# using the softmax function on the final layer\n",
    "loss_activation1=Activation_softmax_LCCE()\n",
    "\n",
    "#So for optimizer we can choose between Adam and SGD optimizer\n",
    "#Creating optimizer\n",
    "optimize1 = Adam_op(l_rate=0.05,decay=5e-7)\n",
    "\n",
    "#Loop to train in\n",
    "for epoch in range(11001):\n",
    "    \n",
    "    #pass input X through first layer\n",
    "    #Output of first layer passes through the firstactivation function\n",
    "    # Perform a forward pass through the activation/loss function\n",
    "    # takes the output of second dense layer here and returns loss\n",
    "    \n",
    "    Hlayer11.fpropogation(input11)\n",
    "    act11.fRelu(Hlayer11.output)\n",
    "    #Hlayer12.fpropogation(act11.output)\n",
    "    #act12.fRelu(Hlayer12.output)\n",
    "    Output_layer1.fpropogation(act11.output)\n",
    "    loss1 = loss_activation1.forward(Output_layer1.output,result11)\n",
    "    \n",
    "    #Argmax returns the predictions for the foward pass\n",
    "    prediction1 = np.argmax(loss_activation1.output,axis=1)\n",
    "    \n",
    "    #calculate values along first axis\n",
    "    if len(result11.shape)==2:\n",
    "        result11=np.argmax(result11,axis=1)\n",
    "    \n",
    "    \n",
    "    #returns the accuracy of our predictions in this pass\n",
    "    accuracy1= np.mean(prediction1==result11)\n",
    "    \n",
    "    #priniting for every 100Th epoch\n",
    "    if not epoch %100:\n",
    "        print(f'epoch:{epoch} ,'+\n",
    "              f'accuracy:{accuracy1:.3f} ,'+\n",
    "              f'loss:{loss1:.3f} ,'+\n",
    "              f'lr:{optimize.curr_l_rate},')\n",
    "        \n",
    "    \n",
    "    \n",
    "    #back propogation \n",
    "    #The values are propogated backwards and the dinputs are updated\n",
    "    loss_activation1.backpass(loss_activation1.output,result11)\n",
    "    Output_layer1.Bpropogation(loss_activation1.dinputs)\n",
    "    act11.back(Output_layer1.dinputs)\n",
    "    #Hlayer12.Bpropogation(act12.dinputs)\n",
    "    #act11.back(Hlayer12.dinputs)\n",
    "    Hlayer11.Bpropogation(act11.dinputs)\n",
    "              \n",
    "    #Update in weights and the biases\n",
    "    optimize.initial_update_param()\n",
    "    optimize.update_params(Hlayer11)\n",
    "    #optimize.update_params(Hlayer12)\n",
    "    optimize.update_params(Output_layer1)\n",
    "    optimize.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:11000 ,accuracy:0.881 ,loss:0.547 ,lr:0.024740215368522826\n"
     ]
    }
   ],
   "source": [
    "x_test=x_test11\n",
    "y_test=y_test11\n",
    "Hlayer11.fpropogation(x_test)\n",
    "act11.fRelu(Hlayer11.output)\n",
    "#Hlayer12.fpropogation(act11.output)\n",
    "#act12.fRelu(Hlayer12.output)\n",
    "Output_layer1.fpropogation(act11.output)\n",
    "loss1 = loss_activation1.forward(Output_layer1.output,y_test)\n",
    "prediction1 = np.argmax(loss_activation1.output,axis=1)\n",
    "if len(y_test.shape)==2:\n",
    "    result11=np.argmax(y_test,axis=1)\n",
    "accuracy1= np.mean(prediction1==y_test)\n",
    "print(f'epoch:{epoch} ,'+f'accuracy:{accuracy1:.3f} ,'+\n",
    "       f'loss:{loss1:.3f} ,'+\n",
    "      f'lr:{optimize.curr_l_rate}')\n",
    "\n",
    "#predict values using model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[13  2]\n",
      " [ 3 24]]\n",
      "\n",
      "\n",
      "Precision : 0.9230769230769231\n",
      "Recall : 0.8888888888888888\n",
      "F1 SCORE : 0.9056603773584906\n"
     ]
    }
   ],
   "source": [
    "model1.CM(y_test.tolist(),prediction1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
